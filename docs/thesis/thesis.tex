\documentclass[spanish,12pt, a4paper,twoside]{paper}

\let\oldsection\section
\def\section{\cleardoublepage\oldsection}

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\input{tweetsccore_javadoc_preamble}

\usepackage[textwidth=15cm, textheight=22.5cm, top=3.5cm, bottom=3.5cm,left= 4cm,right=2cm]{geometry}

\usepackage[spanish, activeacute]{babel}
%\usepackage[applemac]{inputenc} 
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{changepage}
\usepackage{subcaption}

\usepackage{url}
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{algorithm}
\usepackage{multirow}


\begin{document}
%\maketitle
%\thispagestyle{empty}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%	HEADING SECTIONS
\includegraphics[width=2.25cm]{recursos/logoFi.png}
  \hspace{8cm}
\includegraphics[width=2cm]{recursos/logoupm.png}
\\[1cm]

\textsc{\Large Escuela Técnica Superior de Ingenieros Informáticos}\\[0.5cm]
\textsc{\large Universidad Polítecnica de Madrid}
\\[3cm]


%	TITLE SECTION
 \HRule \\[0.4cm]
{ \huge \bfseries TweetSC: Corrector de texto para Twitter}\\[0.4cm] % Title of your document
\HRule \\[2.5cm]

\textsc{\LARGE Trabajo Fin de Máster}\\[0.5cm] 
\textsc{\Large Máster Universitario en Inteligencia Artificial }\\[2.5cm]

 %	AUTHOR SECTION
\begin{flushright}
\large
AUTOR: Javier Moreno Vega\\
TUTOR/ES: Óscar Corcho García y \linebreak
                    Víctor Rodríguez Doncel
\end{flushright}

%\vspace{0.5cm}

%	URL SECTION
{{\url{https://jmorenov.github.io/TweetSC/}}}\\[1cm]

%\vspace{0.5cm}

%	DATE SECTION
{ {\today}}\\[1cm]

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\afterpage{\blankpage}
\pagenumbering{roman}

%	RESUMEN
\section*{RESUMEN}
Esta memoria describe TweetSC, un corrector de texto para mensajes en español en Twitter. Primero se realiza una introducción al tema y se exponen los objetivos a realizar. En segundo lugar se presenta el estado del arte sobre el tema de la corrección de textos, específicamente en Twitter y en español. Posteriormente se explica la solución propuesta con todas sus fases. A continuación se presenta la implementación desarrollada y la documentación del código, además de los recursos utilizados. En siguiente lugar evaluamos los resultados. Y por último se desarrollan las conclusiones y líneas futuras.


%	SUMMARY
\section*{SUMMARY}
This dissertation describes TweetSC, a text corrector for Spanish messages on Twitter. The document goes into detail in the state of the art and the software module developed. First an introduction to the subject is made and the objectives to be made are exposed. Second, the state of the art on the subject of normalization is presented, specifically on Twitter and in Spanish. Later I explain the proposed solution with all its phases. Below is the developed implementation and documentation of the code, in addition to the resources used. Next, we evaluate the results. And finally the conclusions and future lines are developed.


%	ÍNDICE
\tableofcontents % indice de contenidos



%	INDICE DE FIGURAS Y TABLAS
\listoffigures
\listoftables



%	CAPÍTULOS DEL TRABAJO FIN DE MÁSTER
\newpage
\pagenumbering{arabic} 

\section{Introducción}\label{sec:introduccion}
\subsection{Motivación}\label{sec:motivacion}
Los nuevos sistemas de comunicación como la mensajería instantánea, chats, redes sociales han generado un uso diferente de los idiomas en estos ámbitos, llamado lenguaje tipo chat \cite{forsyth:2007}. Una de estas redes sociales y en la que este trabajo va a centrarse es Twitter. En esta red social predomina el uso de emoticonos, repetición de vocales o eliminación de las mismas, uso abusivo de mayúsculas o su asusencia, siglas de expresiones populares; y otras características que dificulta el análisis de los textos. Las ventajas que ofrece esta red social para investigar sobre ella son la existencia  de cantidad de datos en tiempo real y su fácil acceso.\\

Uno de los principales problemas a la hora de analizar textos procedentes de las redes sociales son los errores ortográficos y gramaticales que suelen contener, así como la presencia de elementos propios de este tipo de foros que requieren de un procesamiento especial (i.e. hashtags, formas de mencionar a otros usuarios o emoticonos y expresiones habituales en las redes). Además, la limitación en el número de caracteres existente en Twitter la convierte en un caso singular dentro de las redes sociales, ya que los usuarios tienden a adaptar su forma de escribir a dicha limitación, omitiendo palabras y creando abreviaturas que dificultan el uso de herramientas genéricas de procesamiento del lenguaje, especialmente a la hora de realizar tareas como el Análisis de Sentimientos.\\

Los usuarios en Twitter tienden a cometer errores tipográficos, abreviaciones, sustituciones fonéticas y estructuras no gramaticales en los mensajes cortos de texto, causando problemas en las herramientas de análisis. Esto es lo que se consideran palabras mal formadas y la detección de las palabras mal formadas es difícil debido al contexto ruidoso. El objetivo es normalizar estas palabra mal formadas.\\

A parte de un uso puramente de investigación, este tipo de trabajo también es beneficioso para un estudio de marcas o personas y sobre lo que las persones opinan sobre ello en las redes social, ya que sin el proceso de normalización y análisis de sentimientos estaríamos ante millones de datos que costarían mucho trabajo analizar de una forma automática.

\subsection{Objetivos}\label{sec:objetivos}
El objetivo principal de este trabajo es la creación de un corrector que "normalice" tweets en español.\\Para cumplir con este objetivo principal se ha dividido en los siguientes subobjetivos.
\begin{itemize}
\item Acceso a la API de Twitter para obtener tweets.
\item Tokenizar tweets.
\item Detectar entre los tokens las palabras fuera del vocabulario (Out-of-Vocabulary, OOV).
\item Anotar el tipo de palabras OOV.
\item Corregir palabras OOV. 
\end{itemize}
Estos subojetivos se cumplirán con su implementación en un módulo software que además estará disponible en una aplicación web \cite{tweetscweb}.\\

También ejecutaremos este corrector sobre un corpus de tweets disponible en \cite{tweetnorm} y compararemos nuestros resultados con los que se consiguieron en \cite{alegria:2013}.

\subsection{Resumen del documento}\label{sec:resumen}
Esta memoria explica todo el trabajo desarrollado entrando en detalle en el estado del arte y el  módulo software desarrollado. Primero se realiza una introducción al tema y se exponen los objetivos a realizar. En segundo lugar se presenta el estado del arte sobre el tema de la corrección de textos, específicamente en Twitter y en español. Posteriormente explicamos la solución propuesta con todas sus fases. A continuación se presenta la implementación desarrollada y la documentación del código, además de los recursos utilizados. En siguiente lugar evaluamos los resultados. Y por último se desarrollan las conclusiones y líneas futuras.
\section{Estado del arte}\label{sec:estadodelarte}
\subsection{Introducción}\label{sec:introduccion}
En la actualidad, la normalización lingüística de tweets \cite{baldwin:2011} supone un campo de gran interés y en donde la mayoría de trabajos se han realizado sobre textos en inglés y pocos en español. Además no hay ningún trabajo en donde se incluya, dentro de la normalización de tuits, el estudio de los hashtags o etiquetas y los emoticonos, y su contexto. \\
Una introducción al tema de normalización de tuits es el artículo \cite{eisenstein:2013}, donde se revisa el estado del arte en NLP sobre variantes SMS y tweets, y cómo la comunidad científica ha respondido por dos caminos: normalización y adaptación de herramientas.
\subsection{Normalización}\label{sec:normalizacion}
El modelo de canal ruidoso \cite{shannon:1948} ha sido tradicionalmente la primera aproximación a la normalización de textos. Supone que el texto mal formado es T y su forma normalizada es S, por lo que hay que encontrar: arg max $P(S|T)$, calculando arg max $P(T|S) P(S)$, P(S) es el modelo del lenguaje y $P(T|S)$ es el modelo de error. \cite{brillmoore:2000} caracterizan el modelo de error calculando el producto de operaciones de probabilidad en partes de cadenas de caracteres. \cite{toutanovamoore:2002} mejoraron el modelo incorporando información de la pronunciación. \cite{choudhury:2007} modela el proceso de generación de texto a nivel de palabra para mensajes SMS considerando las abreviaturas grafémicas/fonéticas y los errores tipográficos involuntarios como transiciones de estado ocultas del modelo de Markov (HMM) y emisiones, respectivamente. \cite{cookstevenson:2009} expandieron el modelo de error introduciendo inferencias de diferentes procesos de formación erróneos, de acuerdo con la distribución de errores muestreada.\\

Mientras el modelo de canal ruidoso es apropiado para normalización de textos, es difícil aproximar la normalización con exactitud, además estos métodos ignoran el contexto alrededor del OOV, el cual ayuda a resolver ambigüedades. La traducción automática estadística (SMT) se ha propuesto como un medio de normalización de texto sensible al contexto, al tratar el texto mal formado como el idioma de origen, y la forma estándar como el idioma de destino. Por ejemplo \cite{aw:2006}. Normalización de textos como un problema de reconocimiento de voz \cite{kobus:2008}. \cite{beaufort:2002} métodos de estado finitos combinando las ventajas de SMS y el modelo de canal ruidoso. \cite{kaufmannkalita:2010} usan un enfoque de traducción automática con un preprocesador para la normalización sintáctica (en lugar de léxica).\\

El problema de estos trabajos anteriores es que requieren datos de entrenamiento anotados a gran escala, lo que limita su adaptabilidad a nuevos dominios o idiomas, mientras que los trabajos \cite{whitelaw:2009} y \cite{baldwin:2011}, no. Estos trabajos son una buena referencia en el campo de la normalización de tuits en inglés de forma no supervisada. En donde para detectar palabras fuera de diccionario (OOV) utilizan GNU aspell, y los usuarios (@usuario), los hashtags y las URLs son excluidas de la normalización. La normalización tiene relación con los correctores de texto \cite{peterson:1980} pero difiere en que las palabras mal formadas en los mensajes de texto suelen ser intencionadas, para ahorrar caracteres, como identidad social, o debido a la convención en este subgénero de texto. La detección de las palabras mal formadas es difícil debido al contexto ruidoso. El objetivo es normalizar estas palabra mal formadas, además muchas palabras mal formadas son ambiguas y requieren el contexto para poder normalizarlas.

\subsection{Adaptación de herramientas}\label{sec:adaptaciondeherramientas}
En vez de adaptar el texto a herramientas de análisis otro de los caminos a seguir es adaptar las herramientas de análisis al texto. Destacan los trabajos de reconocimiento de voz \cite{gimpel:2011} \cite{owoputi:2013}, reconocimiento de entidades \cite{finin:2010} \cite{ritter:2011} \cite{liu:2011}, análisis gramatical \cite{foster:2011}, modelización de diálogos \cite{ritter:2010} y resumen automático de textos \cite{sharifi:2010}.\\

El reconocimiento de entidades nombradas (NER) es una tarea de extracción de información que busca localizar y clasificar en categorías predefinidas, como personas, organizaciones, lugares, expresiones de tiempo y cantidades, entidades encontradas en un texto. Las soluciones propuestas para NER suelen recaer en tres categorías: Basado en reglas \cite{krupkahausman:1998}, Basada en aprendizaje automático \cite{finkelmanning:2009} \cite{singh:2010} y Métodos híbridos \cite{jansche:2002}. Con la disponibilidad de datos anotados, Enron \cite{minkov:2005} y CoNLL03 \cite{tjong:2003} se han convertidos en los nuevos métodos dominantes. El estudio actual NER se centra principalmente en textos formales, de hecho, el estado del arte actual (CoNLL03) tiene un éxito del 90.8\% en textos formales y 45.8\% en tweets. En el contexto de los textos en Tweets, existe una dificultad en el reconocimiento de entidades nombradas debido a la falta de información y datos de entrenamiento.\\

El trabajo en el contexto de los textos de Twitter se puede dividir en tres categorías: NER en tweets, NER en no tweets y aprendizaje semi-supervisado para NER. El trabajo principal de NER sobre tweets es \cite{finin:2010}, en donde se anotan los tweets y se entrena el modelo con CRF. En cuanto a los trabajos de NER sobre no tweets: \cite{krupkahausman:1998} utilizan reglas manuales para extraer entidades de tipos predefinidos, \cite{zhousu:2002} utilizan HMM (Hidden Markov Model) mientras que \cite{finkel:2005} usa CRF. En la tercera categoría, aprendizaje semi-supervisado para NER, se encuentran los trabajos de \cite{jiangzhai:2007} que utiliza un algoritmo de bootstrapping balanceado, \cite{wu:2009} también utiliza un algoritmo de bootstrapping, \cite{miller:2004} clusters de palabras, \cite{brown:1992} aprende desde texto sin etiquetar y \cite{guo:2009} introduce Latent Semantic Association (LSA) para NER. El trabajo más importante y actual de NER para tweets es \cite{liu:2011} donde replantea el tema de reconocimiento de entidades nombradas en corpus de tuits. Combina un clasificador KNN con CRF (Conditional Random Fields).\\

La desambiguación léxica o etiquetado gramatical (POST) es una parte muy importante y útil en la tarea de normalización de textos ya que nos permite definir el subconjunto de palabras debido a su categoría gramatical que con una probabilidad pueden ser la normalización de un OOV. Además un gran porcentaje de palabras en un texto son palabras que pueden ser asignadas a más de una clase morfológica, a más de un part-of-speech (PoS). Uno de los trabajos más importantes y probado para español es \cite{sanchezforcada:2004}, este trabajo presenta un método de POST de ventana deslizante (SWPoST), asigna el part-of-speech de una palabra basado en la información que dan las palabras en una ventana fija de alrededor. Puede ser implementado como una máquina de estados finitos (Máquina de Mealy).


\subsection{Normalización en espa'nol}\label{sec:normalzacionenespanol}
Una introducción a la normalización de tuits en español es \cite{alegria:2013}\cite{alegria:2015}. Este trabajo propuso en 2013 una tarea o competición en la que los participantes proponían soluciones de normalización de tweets. Los organizadores de la competición ofrecían dos datasets de tweets ya notados uno de desarrollo y otro para test, junto con un tercero que no era público y que era usado para la última evaluación.\\

Las soluciones ofreciddas por los participantes se pueden dividir en dos categorías, los que utilizan generación de candidatos junto un modelo del lenguaje, y los que utilizan transductores o FSTs (Finite State Transducers). El participante que mejor accurancy consiguió, Sistema RAE \cite{porta:2013} con un 0.781, optó por la segunda categoría e implementó un siststem basado en FSTs para la tarea de normalización léxica de mensajes de Twitter en Español. El sistema desarrollado consiste en transductores que se aplican a tokens OOV. Los transductores implementan modelos de variación lingüística que generan conjuntos de candidatos acordes a un léxico. Un modelo estadístico del lenguaje se usa para obtener la secuencia de palabras más probable. El sistema tiene tres componentes principales que se aplican secuencialmente. Un analizador que ejecuta tokenización y análisis léxico sobre palabras en forma estándar y otras expresiones (números, fechas, …). Un componente que genera palabras candidatas para los tokens OOV. Un modelo estadístico del lenguaje para obtener la mejor secuencia de palabras. Y finalmente un truecaser para capitalizar correctamente las palabras asignadas a los tokens OOV. El conjunto de confusión de un token OOV se genera aplicando el algoritmo de camino mínimo a la expresión: $W \circ E \circ L$. Donde W es el automata que representa el token OOV, E es un transductor de editado que genera todas las posibles variaciones de un token, y L es un conjunto de palabras objetivo. Dentro de esta categoría se encuentran los trabajos de la tarea: \cite{ageno:2013} en donde usan una batería de módulos para generar diferentes propuestas de corrección para cada palabra desconocida. La corrección definitiva se elige por votación ponderada según la precisión de cada módulo, \cite{alegria:2013} que además utiliza un modelo para el reconocimiento de voz para la generación de candidatos y \cite{huldenfrancom:2013} presentan dos estrategias basadas en FSTs una con reglas diseñadas manualmente y la otra automática.\\

Entre los participantes que optaron por la primera categoría destaca \cite{ruizcuadros:2013}\cite{vicomtech} que usa reglas de preproceso, un modelo de distancias de edición adecuado al dominio y modelos de lengua para seleccionar candidatos de corrección según el contexto. Su arquitectura está formada por: preproceso basado en expresiones regulares y listas customizadas, generación de candidatos mediante una técnica de mínima de distancia de editado, ranking de candidatos mediante una combinación con pesos de la puntuación del modelo del lenguaje y la distancia de editado y la puntuación del modelo de lenguaje es n-grama utilizando la distancia Levenshtein. El sistema obtuvo resultados superiores a la media en la tarea. Una mejora a este trabajo por los mismos autores es \cite{ruizcuadros:2014} en donde utilizan un sistema basado en reglas para seleccionar los candidatos. Otros trabajos en esta categoría son: \cite{gamallo:2013} que propone un sistema basado en \cite{han:2013}, \cite{saralegi:2013} y \cite{mosqueralopezmoreda:2013} que emplea técnicas de RAH (reconocimiento del habla) mediante la herramienta TENOR \cite{mosquera:2012} junto con un modelo del lenguaje. Otro trabajo basado en la tarea de Tweet-Norm pero que no participó en ella es \cite{ceronguzman:2016}, ellos optaron por normalizar los OOV basándose en similaridad entre grafemas y fonemas; generan el conjunto de confusión (de candidatos) usando grafemas y fonemas, seguido de transductores aplicados mediante reglas para las palabras extranjeras y acentos, la selección de candidatos mediante un modelo del lenguaje con la herramienta Kenlm \cite{heafield:2011}.\\

Fuera de estas dos categorías nos encontramos con los trabajos: \cite{montejo:2013} que utiliza conversiones basadas en reglas hasta una forma final normalizada. Después de recibir una lista con las posibles correcciones el sistema selecciona la más común acorde con una lista de palabras ordenada por frecuencia, \cite{vilares:2013} utilizan una lista de prioridad para los candidatos obtenidos y una tabla de frencuencias de palabrar para puntuarlos, \cite{han:2013} presentan una estrategia basada en búsquedas rápidas mediante una lista de frecuencias aprendida desde un corpus de tweets, \cite{munozgarcia:2013} no generan candidatos simplemente selecciona palabras OOV y las corrigen con un corrector externo y \cite{cotelo:2013} generan candidatos y seleccionan el mejor mediante una función de distancia. Una mejora a este último trabajo por parte de los autores fue \cite{cotelocruz:2015} donde añaden un modulo de puntuación para la selección de candidatos. \\

Otros trabajos sobre normalización en español son \cite{mosquera:2012} en donde se generan candidatos con indexación fonética y se seleccionan el candidato calculando la similaridad léxica junto con un modelo del lenguaje trigrama y \cite{oliva:2011}. Estos trabajos son principalmente sobre mensajes SMS, y no abordan la normalización de tuits en su conjunto. Dentro de la normalización en español existen otras tareas relacionadas como es la tokenización y aquí destaca el trabajo \cite{gomezhidalgo:2013} que estudia la tokenización de textos SMS.

\subsection{Word2Vec}\label{sec:word2vec}
Muchos sistemas y técnicas actuales de NLP tratan las palabras como unidades atómicas, no hay noción de similaridad entre palabras y son representadas como índices en un vocabulario, por ejemplo el modelo N-grama, para tratar de resolver este problema aparecen las representaciones continuas de palabras. Las representaciones continuas de palabras entrenadas sobre corpus sin etiquetas son útiles para muchos trabajos de NLP. Muchos tipos diferentes de modelos han sido propuestos para estimar representaciones continuas de palabras, incluyendo Latent Semantic Analysis (LSA) y Latent Dirichlet Allocation (LDA). En este trabajo se centran en las representaciones distribuidas de palabras aprendidas por redes neuronales, ya que se demostró que su eficacia era considerablemente mejor que LSA para preservar regularidades lineales entre palabras, LDA además es computacionalmente caro en datasets grandes. Además se ha demostrado las redes neuronales basadas en modelos del lenguaje mejoran significativamente los modelos N-grama \cite{bengio:2003} \cite{mikolov:2011} \cite{schwenk:2007}.\\

Los dos modelos de redes neuronales que destacan basados en modelos del lenguaje son: Feedforward Neural Net Language Model (NNLM) \cite{bengio:2003} y Recurrent Neural Net Language Model (RNNLM) que mejora algunas limitaciones de NNLM. El problema de estos modelos es que con grandes cantidades de datos son muy costotos computacionalmente. Para resolver este problema en el trabajo \cite{mikolov:2013} desarrollado por Google se presentaron dos nuevos modelos de arquitecturas para calcular representaciones continuas de vectores de palabras a partir de grandes datasets, además crearon un framework de bibliotecas llamado Word2Vec \cite{google:word2vec}. El principal objetivo de este trabajo es introducir técnicas que puedan ser usadas para aprender vectores de palabras de gran calidad a partir de grandes datasets con millones de palabras y con millones de palabras en el vocabulario. Decidieron explorar modelos más simples que aunque no puedan representar los datos de forma tan precisa como las redes neuronales pero pueden ser entrenados con muchos más datos de forma más eficiente. Estos modelos son: Continuous Baf-of-Words model (CBOW) similar a NNLM, la capa oculta no-lineal se elimina y la capa de proyección es compartida por todas las palabras; y Continuous Skip-gram model similar a CBOW pero en vez de predecir la palabra actual basándose en el contexto intenta maximizar la clasificación de la palabra basándose en otra palabra de la misma frase.\\

La mayoría de las técnicas de representación continua de vectores de palabras representan cada palabra del vocabulario como un vector distinto, sin parámetros compartidos. En particular se ignora la estructura interna de las palabras lo que es una importante limitación en lenguajes ricos morfológicamente. Para intentar resolver este problema en el trabajo \cite{bojanowski:2017}, desarrollado por Facebook \cite{facebook:fasttext} y llamado fastText, se propone un nuevo enfoque basado en el modelo skipgram \cite{mikolov:2013} donde cada palabra se representa como una bolsa de caracteres n-gramas. Una representación de vector está asociada con cada caracter n-grama, las palabras se representan como la suma de estas representaciones. Al usar una representación de vector distinta para cada palabra, el modelo skipgram ignora la estructura interna de las palabras y en este nuevo trabajo se implementa una función de puntuación diferente para tener en cuenta esta información.

\section{Solución propuesta}\label{sec:solucionpropuesta}
La solución propuesta y a la que hemos llamado TweetSC (Tweet Spell Checker) \cite{tweetscweb} se llegó a ella a partir de varios análisis y evaluaciones que se hicieron con diversas bibliotecas y algoritmos, y todos ellos se pueden encontrar en la solución final para su uso.\\

En la primera versión de nuestra solución se construyó un corrector de texto sencillo basándonos en el creado por Peter Norvig \cite{peternorvig}, el cuál utiliza un diccionario para seleccionar las palabras incorrectas y las corrige mediante el teorema de Bayes usando probabilidades. Se usa la fórmula: $argmax_{c\ \in\  candidates}P(c|w)$, que mediante el teorema de Bayes es equivalente a: $argmax_{c\ \in\  candidates}P(c) P(w|c) / P(w)$, y como P(w) es igual para cada candidato c: $argmax_{c\ \in\  candidates}P(c) P(w|c)$. Esta fórmula trata de seleccionar el candidato de probabilidad máxima para cada palabra. Para calcular la probabilidad se usan dos diccionarios, uno de palabras en Español y otro de nombres propios. Se utilizó esta primera versión como punto de partida para ir creando versiones más avanzadas.\\

El resultado final y por tanto nuestra versión definitiva consiste en un proceso iterativo sobre el tweet que se puede dividir en 6 fases: Tokenización, reglas de preproceso, detección de OOVs, generación de candidatos para cada OOV, ranking de candidatos y postproceso.\\

Además para convertir el sistema en uno más dinámico se ha desarrollado una aplicación web con acceso a la API de Twitter para obtener los tweets mediante querys introducidas en un formulario de nuestra aplicación web.

\subsection{Tokenización}\label{sec:tokenizacion}
Cómo realizan los analizadores léxicos en los compiladores, en la primera fase de nuestro proceso se realiza una tokenización del texto o tweet, un tokenizador genera una salida compuesta de tokens o símbolos.\\

Para mejorar la versión inicial se hizo uso de la biblioteca Stanford NLP \cite{stanfordnlp}. Esta biblioteca creada por Stanford NLP Group ofrece tanto etiquetado gramatical (POS Tagging o POST) cómo deteccion de etiquetas (Named Entity Recognition o NER), nosotros la hemos utilizado para esta fase de tokenización. Además de StanfordNLP para la tokenización hemos utilizado Freeling \cite{freeling}, también se ha añadido al sistema el análisis que ofrece freeling.\\

En esta primera fase se recibe como entrada el texto del tweet y genera una lista de tokens que pasaran a la siguiente fase.

\subsection{Reglas de preprocesado}\label{sec:reglasdepreprocesado}
Una vez que hemos obtenido todos los tokens de un tweet se aplican unas reglas de preproceso para normalizar palabras típicas de la red social, pictogramas, fonogramas, onomatopeyas, números, acrónimos, etc. Tras aplicar estas reglas a los tokens que las acepten, se crean OOVs con estos token, se anotan como variaciones y se eliminan de la lista de tokens para las fases siguientes. Los OOV generados se añaden a la lista final de OOV.

\subsection{Detección de OOV}\label{sec:detecciondeoov}
Esta fase tiene como elementos de entrada los tokens restantes de la fase anterior, y se ejecuta token por token el detector de OOV. Para detectarlos se aplican reglas y se van descartando los tokens que son URLs, usuarios de Twitter, hashtag de Twitter y fechas; los elementos restantes se comparan con tres diccionarios utilizados como recursos: diccionario de español, diccionario de inglés y diccionario de entidades.\\

Los token que se detecten dentro del diccionario de español se descartan como OOV, los que se detecten en el diccionario de inglés se anotan como NoEs (No español o ininteligible) y los que se detecten en el diccionario de entidades se anotan como Correct (palabras correspondientes a una entidad o un nuevo préstamo). Para el resto de token que no han sido aceptados en ninguna regla se crea una lista de OOV y son los que pasaran a la siguiente fase pudiendo al final ser anotados como Variation o NoEs.

\subsection{Generación de candidatos OOV}\label{sec:generaciondecandidatosoov}
La generación de candidatos se puede considerar la primera fase de la corrección en sí, ya que sólo se trabaja con OOV a los que se va a buscar una corrección, en ese caso candidatos para ese OOV.
Esta fase tiene como entrada la lista de OOV que no han sido etiquetados en la fase anterior, es decir, los que pueden ser Variation o NoEs. Para cada OOV se generarán una lista de candidatos con diferentes métodos. Los métodos que hemos utilizado con los nombres que hemos definido son: LevenshteinFST, Metaphone, L\_L, FastTest.

\begin{itemize}
	\item \textbf{LevenshteinFST:} Método que utiliza un FST (Finite State Transducers) para generar variaciones en el OOV con un máximo de distancia de editado según la distancia Levenshtein.
	\item \textbf{Metaphone:} Método que utiliza el algoritmo del metáfono en español \cite{mosquera:2011}. Su funcionamiento consiste en generar los fonemas de todos los diccionarios que hemos utilizado para después comparar el fonema del OOV y seleccionar los de mayor similaridad con los fonemas de los diccionarios.
	\item \textbf{L\_L:} Los candidatos generados con este método son las palabras aceptadas por el lenguaje L(\_L)+.
	\item \textbf{FastText:} Este método hace uso de la biblioteca fastText \cite{facebook:fasttext}, a partir de un modelo generado mediante redes neuronales y representando las palabras de forma continua. Los OOV se convierten a vectores de palabras y se comparan con los vectores del modelo generado para obtener los candidatos más parecidos a partir del la similaridad del coseno.	
\end{itemize}

Estos métodos se ejecutan sobre todos los OOV y generan una lista de candidatos que pasarán a la siguiente fase.

\subsection{Ranking de candidatos}\label{sec:rankingdecandidatos}
El ranking de candidatos es la fase que define la corrección de un OOV, o si no tiene corrección (se anota como NoEs). Para generar este ranking hemos utilizado dos marcadores, uno un modelo del lenguaje N-Gram (Modelo del lenguaje \ref{sec:glosariodeterminos}) mediante la biblioteca OpenNLP \cite{opennlp} y el otro la distancia de editado Damerau-Levenshtein.\\

El marcador N-Gram se realiza mediante la comparación de los candidatos de su puntuación en el modelo del lenguaje, es decir, para cada candidato se calcula su puntuación si fuera el elegido. Para el marcador de la distancia Damerau-Levenshtein se calculan la distancia entre el OOV y cada candidato. Finalmente mediante estas dos puntuaciones se calcula se realiza el ranking de candidatos posicionando primero los candidatos con mejor puntuación en el modelo del lenguaje y menor distancia de editado al OOV.\\

Se ha definido además un umbral mínimo para realizar el ranking, los candidatos que no lo cumplan con los marcadores son eliminados. Al finalizar este proceso para cada OOV se selecciona el mejor candidato del ranking, y se anota como Variation, y si no tuviera candidatos, debido al umbral, se anotan como NoEs.

\subsection{Postproceso}\label{sec:postproceso}
Esta última fase consiste en poner mayúsculas en las palabras que fueran necesarias, así como signos de exclamación e interrogación.

\section{Implementación}\label{sec:implementacion}
En esta sección se pretende explicar toda la implementación software que se ha realizado de nuestra solución. Primero se realizará una introducción comentando lenguajes y herramientas utilizadas. Segundo se explicará dónde encontrar y cómo utilizar nuestro software . Posteriormente la documentación generada del código fuente. Y por último se explicará la aplicación web que se ha desarrollado.

\subsection{Introducción}\label{sec:introduccion}
La implementación se ha realizado en tres módulos o componentes, por una parte tenemos la biblioteca con la funcionalidad necesaria para corregir textos de Twitter, acceder a su API y evaluar los resultados sobre un corpus de tweets; después un modulo que implementa aplicación web y por último otro que ofrece funcionalidad para utilizar la biblioteca desde línea de comandos.\\

El lenguaje principal utilizado en todo el proyecto ha sido Java, con la excepción de Python para los script de evaluación y normalización de archivos de datos, y hemos hecho uso de Google Cloud Engine \cite{googlecloudengine} para que la aplicación web esté disponible para cualquier usuario \cite{tweetscweb:spellchecker}.\\

Nuestro sistema software se ha intentado desarrollar de forma que sea un sistema de procesamiento dinámico pudiendo añadir y quitar funcionalidad de manera sencilla, cualquier algoritmo o método implementado funciona a partir de una clase superior para que se puedan añadir nuevos métodos.\\

El módulo principal del sistema es TweetSCCore que implementa la biblioteca para corregir tweets, acceder a la API de Twitter y evaluar resultados mediante el script ofrecido por Tweet-Norm 2013 \cite{alegria:2013}. Los módulos que funcionan a partir de TweetSCCore son: TweetSCExecutable que implementa la funcionalidad necesaria para ejectar nuestro sistema a partir de línea de comandos, y TweetSCWeb donde se implementa la aplicacion web.\\

Se han implementado dos métodos de corrección o normalización de tweets a los que hemos llamado DictionaryMethod, método preliminar que hace uso de diccionarios y la regla de Bayes, y TweetSCMethod que es nuestro método final con las fases que han sido explicadas.

\begin{figure}[h]
El diagrama de clases del sistema completo se muestra a continuación:
 \includegraphics[width=1.2\textwidth]{recursos/DiagramaDelSistema.png}
\caption{Diagrama del sistema}
\label{fig:diagramadelsistema}
\end{figure}

\newpage
\subsection{Cómo usarlo}\label{sec:comousarlo}
Para utilizar nuestro software primero es necesario tener instalado Git y Java 1.8. Después de bajar el código fuente: 
\begin{verbatim}
git clone https://github.com/jmorenov/TweetSC
\end{verbatim}
Posteriormente se compila el código: 
\begin{verbatim}
cd TweetSC/code/
chmod +x build_all.sh
./build_all.sh
\end{verbatim}
Para ejecutarlo desde línea de comandos:
\begin{verbatim}
java -jar tweetscexecutable-all-v0.5.0-alpha.jar -text Texto de prueba
\end{verbatim}
La ejecución de la evaluación sobre el corpus de Tweet-Norm 2013 \cite{alegria:2013}
\begin{verbatim}
java -jar tweetscexecutable-all-v0.5.0-alpha.jar \
    -workingDirectory evaluation \
    -annotatedFile tweet-norm-dev500_annotated.txt \
    -tweetsFile tweet-norm-dev500.txt \
    -resultFile results-test-dev500.txt \
    -method TweetSCMethod
\end{verbatim}
La ejecución de la aplicación web:
\begin{verbatim}
cd tweetscweb
./gradlew run
\end{verbatim}

\subsection{Aplicación web}\label{sec:aplicacionweb}
En esta sección se explicará el funcionamiento de la aplicación web desarrollada y se mostrarán capturas de pantalla de la misma en funcionamiento.\\

La aplicación web se encuentra en nuestro paquete TweetSCWeb y hace uso del framework Spring Boot. Para el backend se utiliza Java y para el frontend Javascript (Jquery). Se ha intentado realizar un diseño sencillo y fluido usando la biblioteca Bootstrap.\\

\begin{figure}[h]
El funcionamiento de la aplicación web es muy sencillo, tiene una sección principal desde la que se accede a las demás secciones mediante una barra de navegación.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebInicio.png}
\caption{Inicio de la aplicación web}
\label{fig:webinicio}
\end{center}
\end{figure}

\begin{figure}[h]
En la sección siguiente se puede ver un corrector de texto simple.
\begin{center}
\includegraphics[width=0.9\textwidth]{recursos/WebUseIt.png}
\caption{Sección para utilizar el corrector}
\label{fig:webuseit}
\end{center}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_Corrected.png}
\caption{Ejemplo de texto corregido}
\label{fig:webuseitcorrected}
\end{figure}

\begin{figure}[h]
Si accedemos al corrector de uso avanzado podemos buscar tweets mediante texto, usuarios o id del tweet.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse.png}
\caption{Corrector de tweets uso avanzado}
\label{fig:webuseitadvanceduse}
\end{center}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_Loading.png}
\caption{Ejemplo de búsqueda de tweets}
\label{fig:webuseitadvanceduseloading}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_SearchResult.png}
\caption{Ejemplo de tweets encontrados}
\label{fig:webuseitadvancedusesearchresult}
\end{figure}

\begin{figure}[h]
Los tweets encontrados se pueden seleccionar para corregirlos.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_TweetsSelected.png}
\caption{Ejemplo de selección de tweets para corregir}
\label{fig:webuseitadvancedusetweetselected}
\end{center}
\end{figure}

\begin{figure}[h]
Cuando los tweets son corregidos se marcan y se muestran ambas versiones, normalizada y la inicial.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_TweetCorrected.png}
\caption{Ejemplo de tweets corregidos}
\label{fig:webuseitadvancedusetweetcorrected}
\end{center}
\end{figure}

\begin{figure}[h]
La siguiente sección es la de características del sistema.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebFeatures1}
\caption{Sección de características de la aplicación web}
\label{fig:webfeatures1}
\end{center}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebFeatures2}
\caption{Sección de características de la aplicación web}
\label{fig:webfeatures2}
\end{figure}

\begin{figure}[h]
Además se ha añadido una sección para colaborar mediante GitHub en el desarrollo.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebColaborate}
\caption{Sección de colaboración}
\label{fig:webcolaborate}
\end{center}
\end{figure}

\begin{figure}[h]
Por último la sección de contacto.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebContact}
\caption{Sección de contacto}
\label{fig:webcontact}
\end{center}
\end{figure}

\section{Recursos utilizados}\label{sec:recursosutilizados}
Los recursos utilizados por nuestro sistema son variados, desde diccionarios hasta bibliotecas para el desarrollo. Empezando desde el paquete TweetSCCore, se ha utilizado la biblioteca StanfordNLP \cite{stanfordnlp} para la tokenización, además está disponible en el paquete la biblioteca Freeling \cite{freeling}. El acceso a la API de Twitter se realiza mediante la bibliteca para Java Twitter4j. La detección de OOV utiliza tres diccionarios, el diccionario de español proporcionado por la herramienta Aspell, el diccionario de entidades JRC y un diccionario de inglés \cite{englishdictionary}. Los métodos de la generación de candidatos utilizan los recursos: Algoritmo del metáfono \cite{mosquera:2011}, fastText \cite{facebook:fasttext} y la biblioteca liblevenshtein \cite{liblevenshtein} para el FST. El ranking de candidatos utiliza la biblioteca \cite{opennlp} para el modelo del lenguaje N-Grama y la distancia Damerau-Levenshtein ofrecida por String.Util de java.\\

Además el corpus de tweets para la evaluación es el que recopiló Tweet-Norm 2013 \cite{alegria:2013} y su script en Python para los resultados.\\

El paquete TweetSCWeb utiliza el framework Spring Boot para la aplicación web junto con el sdk de Google Cloud para ofrecer la aplicación en la nube.

\section{Evaluación}\label{sec:evaluacion}
Esta sección describe la evaluación que se ha hecho de la solución propuesta. Primero se define la metodología utilizada para evaluar la solución, en segundo lugar explicamos el corpus utilizado como datos de entrada, posteriormente el gold standard actual y por último los experimentos que hemos realizada con sus resultados.

\subsection{Metodología}\label{sec:metodologia}
La metodología que hemos seguido ha sido la misma que en la tarea compartida Tweet-Norm 2013 \cite{tweetnorm}. Ellos utilizan como medida de evaluación la corrección de errores, sólo tiene en cuenta si la forma propuesta es correcta en base a los criterios: \textbf{correcta} si la forma original era correcta y no se ha realizado ninguna normalización o si la forma original era incorrecta y el candidato seleccionado es el correcto; \textbf{errónea} en cualquier otro caso. La evaluación final es el número de decisiones realizadas correctamente sobre el total de palabras OOV.

\subsection{Corpus}\label{sec:corpus}
El corpus utilizado es el mismo que en la tarea compartida Tweet-Norm 2013 \cite{tweetnorm}, en donde utilizan dos subconjuntos uno de desarrollo con 500 tweets y otro de evaluación con 600 tweets.

\subsubsection{Gold Standard}\label{sec:goldstandard}
Nuestro gold standard ha sido el sistema propuesto RAE \cite{porta:2013} en Tweet-Norm 2013 \cite{tweetnorm} donde consiguieron un resultado de 0.781 de precisión. Su sistema se basa en trasductores de estados finitos con pesos. 

\subsection{Experimentos}\label{sec:experimentos}
(Experimentos realizados)

\section{Conclusiones}\label{sec:conclusiones}
El proyecto realizado está compuesto de una parte de investigación, cómo se demuestra con el estado del arte y las diferentes soluciones que se han ido realizando hasta llegar a nuestra solución final. Además de la otra parte de desarrollo e implementación de software, ofrenciéndolo para todos en código abierto y en una aplicación web \cite{tweetscweb}.\\

Este desarrollo software se ha dividido en tres componentes o módulos:
\begin{itemize}
	\item TweetSCCore: Núcleo del proyecto con la funcionalidad para corregir textos de Twitter.
	\item TweetSCWeb: Aplicación web para corregir textos.
	\item TweetSCExecutable: Ejecutable java para corregir textos desde línea de comandos.
\end{itemize}
Se puede concluir que nuestro objetivo era construir un corrector de texto para Twitter (\hyperref[sec:objetivos]{sección 1.2}) y se ha conseguido cómo se ha demostrado en las secciones \hyperref[sec:solucionpropuesta]{sección 3} y \hyperref[sec:implementacion]{sección 4}.

\section{Líneas Futuras}\label{sec:lineasfuturas}
Las líneas futuras son muy ámplias ya que estamos en un tema bastante reciente, sobre todo en español cómo se puede ver en el \hyperref[sec:estadodelarte]{sección 2}, y los resultados se pueden mejorar de bastantes formas. Centrándonos en nuestro sistema una mejora futura sería el añadir contexto a los tweets a partir de sus hashtag, usuarios, imágenes o emoticonos; de forma que se pudiera reducir el conjunto de candidatos o añadir nuevos a partir de estos datos. También se podría realizar un análisis de sentimientos sobre el tweet después de normalizar por si se pudiera mejorar la corrección de algún OOV.

\section{TweetSCCore Documentación del código (Java Documentation)}\label{sec:tweetsccorejavadoc}
\input{tweetsccore_javadoc}

\section{TweetSCWeb Documentación del código (Java Documentation)}\label{sec:tweetscwebjavadoc}
\input{tweetscweb_javadoc}

\section{TweetSCExecutable Documentación del código (Java Documentation)}\label{sec:tweetscexecutablejavadoc}
\input{tweetscexecutable_javadoc}

\section{ANEXOS}

\subsection{Glosario de términos}\label{sec:glosariodeterminos}
\begin{itemize}
	\item \textbf{Modelo (estadístico) del lenguaje:} Un modelo estadístico del lenguaje es una distribución de probabilidad sobre secuencias de palabras. Un tipo de modelo del lenguaje es el unigrama, también se suele llamar modelo de bolsa de palabras. La dispersidad en los datos es un problema al construir modelos del lenguaje. La secuencia de palabras más probable puede no aparecer en los datos de entrenamiento. Una solución es realizar la suposición de que la probabilidad de una palabra sólo depende de las n palabras previas. Esto es conocido como el modelo n-grama, unigrama cuando n=1. Los modelos del lenguaje neuronales o modelos del lenguaje continuos: modelo del lenguaje Skip-gram, base de word2vec.
	\item \textbf{Modelo del lenguaje N-grama:} Un modelo de n-grama es un tipo de modelo probabilístico que permite hacer predicción estadística del próximo elemento de cierta secuencia de elementos sucedida hasta el momento. Un modelo de n-grama puede ser definido por una cadena de Márkov de orden n-1. Predice $x_i$ basándose en los n elementos anteriores. 
	\item \textbf{Cadena de Márkov:} En la teoría de la probabilidad, se conoce como cadena de Márkov o modelo de Márkov a un tipo especial de proceso estocástico discreto en el que la probabilidad de que ocurra un evento depende solamente del evento inmediatamente anterior. En matemáticas se define como un proceso estocástico discreto que cumple con la propiedad de Márkov, es decir, si se conoce la historia del sistema hasta su instante actual, su estado presente resume toda la información relevante para describir en probabilidad su futuro.
	\item \textbf{Proceso de Márkov:} Fenómeno aleatorio dependiente del tiempo para el cual se cumple la propiedad de Márkov. Frecuentemente el término cadena de Márkov se usa para dar a entender que un proceso de Márkov tiene un espacio de estados discreto (infinito o numerable).
	\item \textbf{Modelo oculto de Márkov:} Un modelo oculto de Márkov (Hidden Markov Model, HMM) es un modelo estadístico en el que se asume que el sistema a modelar es un proceso de Márkov de parámetros desconocidos. El objetivo es determinar los parámetros desconocidos (u ocultos) de dicha cadena a partir de los parámetros observables. Un HMM se puede considerar como la red bayesiana más simple. 
	\item \textbf{Etiquetado gramatical:} El etiquetado gramatical (part-of-speech tagging, POS tagging o POST) se considera el proceso de asignar a cada palabra de un texto su categoría gramatical. Las soluciones se pueden dividir en dos grandes grupos: aproximaciones lingüísticas basadas en un conjunto de reglas establecidas manualmente por expertos aprendidas de forma (semi)automática, y las aproximaciones de aprendizaje automático que usan textos, generalmente anotados, para establecer los modelos. Además se pueden encontrar aproximaciones híbridas que combinan ciertos aspectos de las anteriores.

\end{itemize}

%	REFERENCIAS
\bibliographystyle{plain}
\bibliography{References}

\end{document}