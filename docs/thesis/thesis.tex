\documentclass[spanish,12pt, a4paper,twoside]{paper}

\let\oldsection\section
\def\section{\cleardoublepage\oldsection}

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\input{tweetsccore_javadoc_preamble}

\usepackage[textwidth=15cm, textheight=22.5cm, top=3.5cm, bottom=3.5cm,left= 4cm,right=2cm]{geometry}

\usepackage[spanish, activeacute]{babel}
%\usepackage[applemac]{inputenc} 
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{changepage}
\usepackage{subcaption}

\usepackage{url}
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{algorithm}
\usepackage{multirow}


\begin{document}
%\maketitle
%\thispagestyle{empty}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%	HEADING SECTIONS
\includegraphics[width=2.25cm]{recursos/logoFi.png}
  \hspace{8cm}
\includegraphics[width=2cm]{recursos/logoupm.png}
\\[1cm]

\textsc{\Large Escuela Técnica Superior de Ingenieros Informáticos}\\[0.5cm]
\textsc{\large Universidad Polítecnica de Madrid}
\\[3cm]


%	TITLE SECTION
 \HRule \\[0.4cm]
{ \huge \bfseries TweetSC: Corrector de texto para Twitter}\\[0.4cm] % Title of your document
\HRule \\[2.5cm]

\textsc{\LARGE Trabajo Fin de Máster}\\[0.5cm] 
\textsc{\Large Máster Universitario en Inteligencia Artificial }\\[2.5cm]

 %	AUTHOR SECTION
\begin{flushright}
\large
AUTOR: Javier Moreno Vega\\
TUTOR/ES: Óscar Corcho García y \linebreak
                    Víctor Rodríguez Doncel
\end{flushright}

%\vspace{0.5cm}

%	URL SECTION
{{\url{https://jmorenov.github.io/TweetSC/}}}\\[1cm]

%\vspace{0.5cm}

%	DATE SECTION
{ {\today}}\\[1cm]

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\afterpage{\blankpage}
\pagenumbering{roman}

%	RESUMEN
\section*{RESUMEN}
Esta memoria describe TweetSC, un corrector de texto para mensajes en español en Twitter. Debido a que los nuevos sistemas de comunicación han generado un uso diferente del idioma, ha surgido un problema en el análisis de textos. Entre todas las redes sociales este trabajo se centra en Twitter debido a las características que tiene. El objetivo principal de este trabajo es la creación de un corrector para tweets en español. El estado del arte sobre este tema no es muy amplío, y en español aún menos; los enfoques que se suelen seguir son los de normalización y adaptación de herramientas, aunque en los últimos años ha surgido un nuevo enfoque basado en redes neuronales y vectores de palabras. Mi solución, a la que he llamado ``Tweet Spell Checker'' (TweetSC),consiste en un sistema basado en modulos que se ejecutan de forma secuencial, desde que entra el texto sin normalizar se va aplicando módulo a módulo hasta que se devuelve el texto normalizado. Los módulos construidos son: Tokenizador, Reglas de preproceso, Detector de  palabras OOV (Out-Of-Vocabulary), generador de candidatos para cada OOV, ranking de candidatos y postproceso; además cada modulo implementa de forma interna varios métodos que se pueden quitar o añadir. La implementación se ha desarrollado en Java y se ha dividido en tres paquetes: tweetsccore (núcleo del sistema y que funciona como una biblioteca por sí sola), tweetscexecutable (paquete que genera un ejecutable jar para su uso desde línea de comandos) y tweetscweb (aplicación web \footnote{\url{https://jmorenov.github.io/TweetSC/}}). Los resultados se han comparado con los de Tweet-Nor 2013 \cite{alegria:2013}. He conseguido unos resultados de accurancy en general bajos pero con posibilidades de mejora y líneas futuras. 


%	SUMMARY
\section*{SUMMARY}
This dissertation describes TweetSC, a text corrector for Spanish messages on Twitter. Because the new communication systems have generated a different use of the language, a problem has arisen in the analysis of texts. Among all social networks, this work focuses on Twitter due to the characteristics it has. The main objective of this work is the creation of a corrector for tweets in Spanish. The state of the art on this subject is not very extensive, and in Spanish even less; the approaches that are usually followed are those of standardization and adaptation of tools, although in recent years a new approach based on neural networks and word vectors has emerged. My solution, which I have called `` Tweet Spell Checker '' (TweetSC), consists of a system based on modules that are executed sequentially, since the text enters without normalizing it is applied module to module until it is returned the normalized text. The built modules are: Tokenizer, Preprocessing rules, OOV (Out-Of-Vocabulary) word detector, generator of candidates for each OOV, ranking of candidates and post-processing; In addition, each module internally implements several methods that can be removed or added. The implementation has been developed in Java and has been divided into three packages: tweetsccore (core of the system and that works as a library by itself), tweetscexecutable (package that generates an executable jar for use from the command line) and tweetscweb (web Application \footnote{\url{https://jmorenov.github.io/TweetSC/}})). The results have been compared with those of Tweet-Nor 2013 \cite{alegria:2013}. I have achieved generally low accurancy results but with possibilities for improvement and future lines.


%	ÍNDICE
\tableofcontents % indice de contenidos



%	INDICE DE FIGURAS Y TABLAS
\listoffigures
\listoftables



%	CAPÍTULOS DEL TRABAJO FIN DE MÁSTER
\newpage
\pagenumbering{arabic} 

\section{Introducción}\label{sec:introduccion}
\subsection{Motivación}\label{sec:motivacion}
\begin{figure}[h]
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/TweetBad.png}
\caption{EJemplo de tweet mal escrito.}
\label{fig:tweetbad}
\end{center}
\end{figure}

Los nuevos sistemas de comunicación como la mensajería instantánea, los chats, las redes sociales han generado un uso diferente de los idiomas en estos ámbitos, llamado lenguaje tipo chat \cite{forsyth:2007}. Una de estas redes sociales y en la que este trabajo va a centrarse es Twitter. En esta red social predomina el uso de emoticonos, repetición de vocales o eliminación de las mismas, uso abusivo de mayúsculas o su asusencia, siglas de expresiones populares; y otras características que dificulta el análisis de los textos. Las ventajas que ofrece esta red social para investigar sobre ella son la existencia  de cantidad de datos en tiempo real y su fácil acceso.\\

Uno de los principales problemas a la hora de analizar textos procedentes de las redes sociales son los errores ortográficos y gramaticales que suelen contener, así como la presencia de elementos propios de este tipo de foros que requieren de un procesamiento especial (i.e. \textit{hashtags}, formas de mencionar a otros usuarios o emoticonos y expresiones habituales en las redes). Además, la limitación en el número de caracteres existente en Twitter la convierte en un caso singular dentro de las redes sociales, ya que los usuarios tienden a adaptar su forma de escribir a dicha limitación, omitiendo palabras y creando abreviaturas que dificultan el uso de herramientas genéricas de procesamiento del lenguaje, especialmente a la hora de realizar tareas como el Análisis de Sentimientos.\\

Los usuarios en Twitter tienden a cometer errores tipográficos, utilizar abreviaturas (tfno.), abreviaciones(telefon), realizar sustituciones fonéticas y emplear estructuras no gramaticales en los mensajes cortos de texto, dificultando la tarea de las herramientas de análisis. \\

Esto es lo que se consideran palabras mal formadas. La detección de las palabras mal formadas es difícil debido al contexto ruidoso y limitado de Twitter, y su normalización es el objetivo de este trabajo. \\

La tarea de normalización también es beneficiosa para un estudio de marcas o personas y sobre lo que las persones opinan sobre ello en las redes social, ya que sin el proceso de normalización y análisis de sentimientos estaríamos ante millones de datos que costarían mucho trabajo analizar de una forma automática.

\subsection{Objetivos}\label{sec:objetivos}
El objetivo principal de este trabajo es la creación de un corrector que ``normalice'' tweets en español.\\Para cumplir con este objetivo principal se ha dividido en los siguientes subobjetivos.
\begin{itemize}
\item Acceso a la API de Twitter para obtener tweets.
\item Tokenización de tweets.
\item Detección entre los tokens las palabras fuera del vocabulario (\textit{Out-of-Vocabulary}, OOV).
\item Anotar el tipo de palabras OOV.
\item Corrección de palabras OOV. 
\end{itemize}
Estos subobjetivos se han cumplido con su implementación en un módulo software que además estará disponible en una aplicación web \cite{tweetscweb}.\\

También he ejecutado este corrector sobre un corpus de tweets \footnote{http://komunitatea.elhuyar.eus/tweet-norm/} y se han comparado los resultados con los que se consiguieron en Tweet-Norm 2013 \cite{alegria:2013}.

\subsection{Resumen del documento}\label{sec:resumen}
Esta memoria explica todo el trabajo desarrollado entrando en detalle en el estado del arte y el  módulo software desarrollado. Primero se realiza una introducción al tema y se exponen los objetivos a realizar. En segundo lugar se presenta el estado del arte sobre el tema de la corrección de textos, específicamente en Twitter y en español. Posteriormente explicamos la solución propuesta con todas sus fases. A continuación se presenta la implementación desarrollada y la documentación del código, además de los recursos utilizados. En siguiente lugar evaluamos los resultados. Y por último se desarrollan las conclusiones y líneas futuras.
\section{Estado del arte}\label{sec:estadodelarte}
\subsection{Introducción}\label{sec:introduccion}
En la actualidad, la normalización lingüística de tweets \cite{baldwin:2011} supone un campo de gran interés y en donde la mayoría de trabajos se han realizado sobre textos en inglés y pocos en español. Además no hay ningún trabajo en donde se incluya, dentro de la normalización de tweets, el estudio de los \textit{hashtag} o etiquetas y los emoticonos, y su contexto. \\
Una introducción al tema de normalización de tweets es el artículo de Eisenstein \cite{eisenstein:2013}, donde se revisa el estado del arte en NLP sobre variantes SMS y tweets, y cómo la comunidad científica ha respondido por dos caminos: normalización y adaptación de herramientas.
\subsection{Normalización}\label{sec:normalizacion}
El modelo de canal ruidoso \cite{shannon:1948} ha sido tradicionalmente la primera aproximación a la normalización de textos. Supone que el texto mal formado es T y su forma normalizada es S, por lo que hay que encontrar: arg max $P(S|T)$, calculando arg max $P(T|S) P(S)$, P(S) es el modelo del lenguaje y $P(T|S)$ es el modelo de error. Brill y Moore \cite{brillmoore:2000} caracterizan el modelo de error calculando el producto de operaciones de probabilidad en partes de cadenas de caracteres. Toutanova y Moore \cite{toutanovamoore:2002} mejoraron el modelo incorporando información de la pronunciación. Choudhury et al. \cite{choudhury:2007} modelan el proceso de generación de texto a nivel de palabra para mensajes SMS considerando las abreviaturas grafémicas/fonéticas y los errores tipográficos involuntarios como transiciones de estado ocultas del modelo de Markov (HMM) y emisiones, respectivamente. Cook y Stevenson \cite{cookstevenson:2009} expandieron el modelo de error introduciendo inferencias de diferentes procesos de formación erróneos, de acuerdo con la distribución de errores muestreada.\\

Mientras el modelo de canal ruidoso es apropiado para normalización de textos, es difícil aproximar la normalización con exactitud, además estos métodos ignoran el contexto alrededor del OOV, el cual ayuda a resolver ambigüedades. La traducción automática estadística (SMT) se ha propuesto como un medio de normalización de texto sensible al contexto, al tratar el texto mal formado como el idioma de origen, y la forma estándar como el idioma de destino. Por ejemplo Aw et al. \cite{aw:2006}. Kobus et al. consideraron la normalización de textos como un problema de reconocimiento de voz \cite{kobus:2008}. Beaufort et al. \cite{beaufort:2002} métodos de estado finitos combinando las ventajas de SMS y el modelo de canal ruidoso. Kaufmann y Kalita \cite{kaufmannkalita:2010} usan un enfoque de traducción automática con un preprocesador para la normalización sintáctica (en lugar de léxica).\\

El problema de estos trabajos anteriores es que requieren datos de entrenamiento anotados a gran escala, lo que limita su adaptabilidad a nuevos dominios o idiomas, mientras que los trabajos: Whitelaw et al. \cite{whitelaw:2009} y Han et al.\cite{baldwin:2011}, no. Estos trabajos son una buena referencia en el campo de la normalización de tweets en inglés de forma no supervisada. En donde para detectar palabras fuera de diccionario (OOV) utilizan la GNU aspell \footnote{\url{http://aspell.net/}}, y los usuarios (@usuario), los \textit{hashtags} y las URLs son excluidas de la normalización. La normalización tiene relación con los correctores de texto \cite{peterson:1980} pero difiere en que las palabras mal formadas en los mensajes de texto suelen ser intencionadas, para ahorrar caracteres, como identidad social, o debido a la convención en este subgénero de texto. La detección de las palabras mal formadas es difícil debido al contexto ruidoso. El objetivo es normalizar estas palabra mal formadas, además muchas palabras mal formadas son ambiguas y requieren el contexto para poder normalizarlas.

\subsection{Adaptación de herramientas}\label{sec:adaptaciondeherramientas}
En vez de adaptar el texto a herramientas de análisis otro de los caminos a seguir es adaptar las herramientas de análisis al texto. Destacan los trabajos de reconocimiento de voz de Gimpel et al. \cite{gimpel:2011} y Owoputi et al. \cite{owoputi:2013}; reconocimiento de entidades: Finin et al. \cite{finin:2010}, Ritter et al. \cite{ritter:2011}, Liu et al. \cite{liu:2011}; análisis gramatical: Foster et al. \cite{foster:2011}; modelización de diálogos: \cite{ritter:2010} y resumen automático de textos de Sharifi et al. \cite{sharifi:2010}.\\

El reconocimiento de entidades nombradas (NER) es una tarea de extracción de información que busca localizar y clasificar en categorías predefinidas, como personas, organizaciones, lugares, expresiones de tiempo y cantidades, entidades encontradas en un texto. Las soluciones propuestas para NER suelen recaer en tres categorías: \textit{Basado en reglas} \cite{krupkahausman:1998}, \textit{Basada en aprendizaje automático} \cite{finkelmanning:2009} \cite{singh:2010} y \textit{Métodos híbridos} \cite{jansche:2002}. Con la disponibilidad de datos anotados, Enron \cite{minkov:2005} y CoNLL03 \cite{tjong:2003} se han convertidos en los nuevos métodos dominantes. El estudio actual NER se centra principalmente en textos formales, de hecho, el estado del arte actual (CoNLL03) tiene un éxito del 90.8\% en textos formales y 45.8\% en tweets. En el contexto de los textos en tweets, existe una dificultad en el reconocimiento de entidades nombradas debido a la falta de información y datos de entrenamiento.\\

El trabajo principal de NER sobre tweets es Finin et al. \cite{finin:2010}, en donde se anotan los tweets y se entrena el modelo con CRF. En cuanto a los trabajos de NER sobre no tweets: Krupka y Hausman\cite{krupkahausman:1998} utilizan reglas manuales para extraer entidades de tipos predefinidos, Zhou y Su \cite{zhousu:2002} utilizan HMM (Hidden Markov Model) mientras que Finkel et al. \cite{finkel:2005} usa CRF. El trabajo más importante y actual de NER para tweets es Liu et al. \cite{liu:2011} donde replantea el tema de reconocimiento de entidades nombradas en corpus de tweets y combina un clasificador KNN con CRF (Conditional Random Fields) para la detección.\\

La desambiguación léxica o etiquetado gramatical (POST) es una parte muy importante y útil en la tarea de normalización de textos ya que nos permite definir el subconjunto de palabras debido a su categoría gramatical que con una probabilidad pueden ser la normalización de un OOV. Además un gran porcentaje de palabras en un texto son palabras que pueden ser asignadas a más de una clase morfológica, a más de un part-of-speech (PoS). Uno de los trabajos más importantes y probado para español es el de Sánchez-Villamil et al. \cite{sanchezforcada:2004}, este trabajo presenta un método de POST de ventana deslizante (SWPoST), asigna el part-of-speech de una palabra basado en la información que dan las palabras en una ventana fija de alrededor. Este algoritmo puede ser implementado como una máquina de estados finitos (Máquina de Mealy).


\subsection{Normalización en espa'nol}\label{sec:normalzacionenespanol}
Una introducción a la normalización de tweets en español se presentó en la tarea Tweet Norm 2013 \cite{alegria:2013}\cite{alegria:2015}. Este trabajo propuso en 2013 una tarea o competición en la que los participantes proponían soluciones de normalización de tweets. Los organizadores de la competición ofrecían dos datasets de tweets ya notados uno de desarrollo y otro para test, junto con un tercero que no era público y que era usado para la última evaluación.\\

Las soluciones ofrecidas por los participantes se pueden dividir en dos categorías, los que utilizan generación de candidatos junto un modelo del lenguaje, y los que utilizan transductores o FSTs (Finite State Transducers). El participante que mejor accurancy consiguió, Sistema RAE \cite{porta:2013} con un 0.781, optó por la segunda categoría e implementó un siststem basado en FSTs para la tarea de normalización léxica de mensajes de Twitter en Español. El sistema desarrollado consiste en transductores que se aplican a tokens OOV. Los transductores implementan modelos de variación lingüística que generan conjuntos de candidatos acordes a un léxico. Un modelo estadístico del lenguaje se usa para obtener la secuencia de palabras más probable. El sistema tiene tres componentes principales que se aplican secuencialmente. Un analizador que ejecuta tokenización y análisis léxico sobre palabras en forma estándar y otras expresiones (números, fechas, …). Un componente que genera palabras candidatas para los tokens OOV. Un modelo estadístico del lenguaje para obtener la mejor secuencia de palabras. Y finalmente un truecaser para capitalizar correctamente las palabras asignadas a los tokens OOV. El conjunto de confusión de un token OOV se genera aplicando el algoritmo de camino mínimo a la expresión: $W \circ E \circ L$. Donde W es el automata que representa el token OOV, E es un transductor de editado que genera todas las posibles variaciones de un token, y L es un conjunto de palabras objetivo. Dentro de esta categoría se encuentran los trabajos de la tarea: Ageno et al. \cite{ageno:2013} que usan una batería de módulos para generar diferentes propuestas de corrección para cada palabra desconocida. La corrección definitiva se elige por votación ponderada según la precisión de cada módulo, Alegria et al. \cite{alegria:2013} que además utilizan un modelo para el reconocimiento de voz para la generación de candidatos y Hulden y Francom \cite{huldenfrancom:2013} presentan dos estrategias basadas en FSTs una con reglas diseñadas manualmente y la otra automática.\\

Entre los participantes que optaron por la primera categoría destacan Ruiz et al. \cite{ruizcuadros:2013} con su sistema Vicomtech \footnote{\url{https://github.com/pruizf/tweet-norm-es}} que usan reglas de preproceso, un modelo de distancias de edición adecuado al dominio y modelos de lengua para seleccionar candidatos de corrección según el contexto. Su arquitectura está formada por: preproceso basado en expresiones regulares y listas customizadas, generación de candidatos mediante una técnica de mínima de distancia de editado, ranking de candidatos mediante una combinación con pesos de la puntuación del modelo del lenguaje y la distancia de editado y la puntuación del modelo de lenguaje es n-grama utilizando la distancia Levenshtein. El sistema obtuvo resultados superiores a la media en la tarea. En una mejora a este trabajo por los mismos autores utilizan un sistema basado en reglas para seleccionar los candidatos \cite{ruizcuadros:2014}. Otros trabajos en esta categoría son: Gamallo et al. \cite{gamallo:2013} que propone un sistema basado en Han et al. \cite{han:2013}, Saralegi y Sa Vicente Rocal \cite{saralegi:2013} y Mosquera y Moreda\cite{mosqueralopezmoreda:2013} que emplean técnicas de RAH (reconocimiento del habla) mediante la herramienta TENOR presentada en Mosquera et al. \cite{mosquera:2012} junto con un modelo del lenguaje. Otro trabajo basado en la tarea de Tweet-Norm pero que no participó en ella es el de Cerón-Guzmán y León-Guzmán\cite{ceronguzman:2016}, ellos optaron por normalizar los OOV basándose en similaridad entre grafemas y fonemas; generan el conjunto de confusión (de candidatos) usando grafemas y fonemas, seguido de transductores aplicados mediante reglas para las palabras extranjeras y acentos, la selección de candidatos mediante un modelo del lenguaje con la herramienta Kenlm \cite{heafield:2011}.\\

Fuera de estas dos categorías nos encontramos con los trabajos: Montejo et al. \cite{montejo:2013} que utilizan conversiones basadas en reglas hasta una forma final normalizada. Después de recibir una lista con las posibles correcciones el sistema selecciona la más común acorde con una lista de palabras ordenada por frecuencia, Vilares et al. \cite{vilares:2013} utilizan una lista de prioridad para los candidatos obtenidos y una tabla de frencuencias de palabrar para puntuarlos, Han et al. \cite{han:2013} presentan una estrategia basada en búsquedas rápidas mediante una lista de frecuencias aprendida desde un corpus de tweets, Muñoz-García et al. \cite{munozgarcia:2013} no generan candidatos simplemente seleccionan palabras OOV y las corrigen con un corrector externo y Cotelo et al. \cite{cotelo:2013} generan candidatos y seleccionan el mejor mediante una función de distancia. En una mejora a este último trabajo por parte de los autores se añade un modulo de puntuación para la selección de candidatos \cite{cotelocruz:2015}. \\

Otros trabajos sobre normalización en español son Mosquera et al. \cite{mosquera:2012} en donde se generan candidatos con indexación fonética y se seleccionan el candidato calculando la similaridad léxica junto con un modelo del lenguaje trigrama y Oliva et al. \cite{oliva:2011}. Estos trabajos son principalmente sobre mensajes SMS, y no abordan la normalización de tweets en su conjunto. Dentro de la normalización en español existen otras tareas relacionadas como es la tokenización y aquí destaca el trabajo Gomez-Hidalgo et al. \cite{gomezhidalgo:2013} que estudia la tokenización de textos SMS.

\subsection{Word2Vec}\label{sec:word2vec}
Muchos sistemas y técnicas actuales de NLP tratan las palabras como unidades atómicas, no hay noción de similaridad entre palabras y son representadas como índices en un vocabulario, por ejemplo el modelo N-grama, para tratar de resolver este problema aparecen las representaciones continuas de palabras. Las representaciones continuas de palabras entrenadas sobre corpus sin etiquetas son útiles para muchos trabajos de NLP. Muchos tipos diferentes de modelos han sido propuestos para estimar representaciones continuas de palabras, incluyendo Latent Semantic Analysis (LSA) y Latent Dirichlet Allocation (LDA). En este trabajo se centran en las representaciones distribuidas de palabras aprendidas por redes neuronales, ya que se demostró que su eficacia era considerablemente mejor que LSA para preservar regularidades lineales entre palabras, LDA además es computacionalmente caro en datasets grandes. Además se ha demostrado las redes neuronales basadas en modelos del lenguaje mejoran significativamente los modelos N-grama \cite{bengio:2003} \cite{mikolov:2011} \cite{schwenk:2007}.\\

Los dos modelos de redes neuronales que destacan basados en modelos del lenguaje son: Feedforward Neural Net Language Model (NNLM) \cite{bengio:2003} y Recurrent Neural Net Language Model (RNNLM) que mejora algunas limitaciones de NNLM. El problema de estos modelos es que con grandes cantidades de datos son muy costotos de obtener. Para resolver este problema en el trabajo de Mikolov et al. \cite{mikolov:2013} desarrollado por Google se presentaron dos nuevos modelos de arquitecturas para calcular representaciones continuas de vectores de palabras a partir de grandes datasets, además crearon un framework de bibliotecas llamado Word2Vec \footnote{\url{https://github.com/deeplearning4j/deeplearning4j}}. El principal objetivo de este trabajo es introducir técnicas que puedan ser usadas para aprender vectores de palabras de gran calidad a partir de grandes datasets con millones de palabras y con millones de palabras en el vocabulario. Se trabajó con modelos más simples que aunque no puedan representar los datos de forma tan precisa como las redes neuronales pero pueden ser entrenados con muchos más datos de forma más eficiente. Estos modelos son: Continuous Baf-of-Words model (CBOW) similar a NNLM, la capa oculta no-lineal se elimina y la capa de proyección es compartida por todas las palabras; y Continuous Skip-gram model similar a CBOW pero en vez de predecir la palabra actual basándose en el contexto intenta maximizar la clasificación de la palabra basándose en otra palabra de la misma frase.\\

La mayoría de las técnicas de representación continua de vectores de palabras representan cada palabra del vocabulario como un vector distinto, sin parámetros compartidos. En particular se ignora la estructura interna de las palabras lo que es una importante limitación en lenguajes ricos morfológicamente. Para intentar resolver este problema en el trabajo Bojanowski et al. \cite{bojanowski:2017}, desarrollado por Facebook y llamado fastText \footnote{\url{https://fasttext.cc/}}, se propone un nuevo enfoque basado en el modelo skipgram \cite{mikolov:2013} donde cada palabra se representa como una bolsa de caracteres n-gramas. Una representación de vector está asociada con cada caracter n-grama, las palabras se representan como la suma de estas representaciones. Al usar una representación de vector distinta para cada palabra, el modelo skipgram ignora la estructura interna de las palabras y en este nuevo trabajo se implementa una función de puntuación diferente para tener en cuenta esta información.

\section{Solución propuesta}\label{sec:solucionpropuesta}
La solución propuesta y a la que he llamado TweetSC (Tweet Spell Checker) \cite{tweetscweb} se llegó a ella a partir de varios análisis y evaluaciones que se hicieron con diversas bibliotecas y algoritmos, y todos ellos se pueden encontrar en la solución final para su uso.\\

En la primera versión de mi solución se construyó un corrector de texto sencillo basándonos en el creado por Peter Norvig \cite{peternorvig}, el cuál utiliza un diccionario para seleccionar las palabras incorrectas y las corrige mediante el teorema de Bayes usando probabilidades. Se usa la fórmula: $argmax_{c\ \in\  candidates}P(c|w)$, que mediante el teorema de Bayes es equivalente a: $argmax_{c\ \in\  candidates}P(c) P(w|c) / P(w)$, y como P(w) es igual para cada candidato c: $argmax_{c\ \in\  candidates}P(c) P(w|c)$. Esta fórmula trata de seleccionar el candidato de probabilidad máxima para cada palabra. Para calcular la probabilidad se usan dos diccionarios, uno de palabras en Español \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/dic.txt}} y otro de nombres propios \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/nombres_propios.txt}}. Se utilizó esta primera versión como punto de partida para ir creando versiones más avanzadas.\\

El resultado final y por tanto nuestra versión definitiva consiste en un proceso iterativo sobre el tweet que se puede dividir en 6 fases: Tokenización, reglas de preproceso, detección de OOVs, generación de candidatos para cada OOV, ranking de candidatos y postproceso.\\

Además para convertir el sistema en uno más dinámico se ha desarrollado una aplicación web con acceso a la API de Twitter para obtener los tweets mediante consultas introducidas en un formulario de nuestra aplicación web.

\subsection{Tokenización}\label{sec:tokenizacion}
Cómo realizan los analizadores léxicos en los compiladores, en la primera fase de nuestro proceso se realiza una tokenización del texto o tweet.\\

Para mejorar la versión inicial se hizo uso de la biblioteca Stanford NLP \cite{stanfordnlp}. Esta biblioteca creada por Stanford NLP Group ofrece tanto etiquetado gramatical (POS Tagging o POST) cómo deteccion de etiquetas (Named Entity Recognition o NER), yo la he utilizado para esta fase de tokenización. Además de StanfordNLP para la tokenización se ha utilizado Freeling \cite{freeling}, también se ha añadido al sistema el análisis que ofrece freeling.\\

En esta primera fase se recibe como entrada el texto del tweet y genera una lista de tokens que pasaran a la siguiente fase.

\subsection{Reglas de preprocesado}\label{sec:reglasdepreprocesado}
Una vez que he obtenido todos los tokens de un tweet se aplican unas reglas de preproceso para normalizar palabras típicas de la red social, pictogramas, fonogramas, onomatopeyas, números, acrónimos, etc. Tras aplicar estas reglas a los tokens que las acepten, se crean OOVs con estos token, se anotan como variaciones y se eliminan de la lista de tokens para las fases siguientes. Los OOV generados se añaden a la lista final de OOV.

\subsection{Detección de OOV}\label{sec:detecciondeoov}
Esta fase tiene como elementos de entrada los tokens restantes de la fase anterior, y se ejecuta token por token el detector de OOV. Para detectarlos se aplican reglas y se van descartando los tokens que son URLs, usuarios de Twitter, \textit{hashtags} de Twitter y fechas; los elementos restantes se comparan con tres diccionarios utilizados como recursos: diccionario de español, diccionario de inglés y diccionario de entidades.\\

Los token que se detecten dentro del diccionario de español se descartan como OOV, los que se detecten en el diccionario de inglés se anotan como NoEs (No español o ininteligible) y los que se detecten en el diccionario de entidades se anotan como Correct (palabras correspondientes a una entidad o un nuevo préstamo). Para el resto de token que no han sido aceptados en ninguna regla se crea una lista de OOV y son los que pasaran a la siguiente fase pudiendo al final ser anotados como Variation o NoEs.

\subsection{Generación de candidatos OOV}\label{sec:generaciondecandidatosoov}
La generación de candidatos se puede considerar la primera fase de la corrección en sí, ya que sólo se trabaja con OOV a los que se va a buscar una corrección, en ese caso candidatos para ese OOV.
Esta fase tiene como entrada la lista de OOV que no han sido etiquetados en la fase anterior, es decir, los que pueden ser Variation o NoEs. Para cada OOV se generarán una lista de candidatos con diferentes métodos. Los métodos que he utilizado con los nombres que he definido son: LevenshteinFST, Metaphone, L\_L, FastTest.

\begin{itemize}
	\item \textbf{LevenshteinFST:} Método que utiliza un FST (Finite State Transducers) para generar variaciones en el OOV con un máximo de distancia de editado según la distancia Levenshtein.
	\item \textbf{Metaphone:} Método que utiliza el algoritmo del metáfono en español \cite{mosquera:2011}. Su funcionamiento consiste en generar los fonemas de todos los diccionarios que he utilizado para después comparar el fonema del OOV y seleccionar los de mayor similaridad con los fonemas de los diccionarios.
	\item \textbf{L\_L:} Los candidatos generados con este método son las palabras aceptadas por el lenguaje L(\_L)+.
	\item \textbf{FastText:} Este método hace uso de la biblioteca fastText \cite{facebook:fasttext}, a partir de un modelo generado mediante redes neuronales y representando las palabras de forma continua. Los OOV se convierten a vectores de palabras y se comparan con los vectores del modelo generado para obtener los candidatos más parecidos a partir del la similaridad del coseno.	
\end{itemize}

Estos métodos se ejecutan sobre todos los OOV y generan una lista de candidatos que pasarán a la siguiente fase.

\subsection{Ranking de candidatos}\label{sec:rankingdecandidatos}
El ranking de candidatos es la fase que define la corrección de un OOV, o si no tiene corrección (se anota como NoEs). Para generar este ranking he utilizado dos marcadores, uno un modelo del lenguaje N-Gram (Modelo del lenguaje \ref{sec:glosariodeterminos}) mediante la biblioteca OpenNLP \cite{opennlp} y el otro la distancia de editado Damerau-Levenshtein.\\

El marcador N-Gram se realiza mediante la comparación de los candidatos de su puntuación en el modelo del lenguaje, es decir, para cada candidato se calcula su puntuación si fuera el elegido. Para el marcador de la distancia Damerau-Levenshtein se calculan la distancia entre el OOV y cada candidato. Finalmente mediante estas dos puntuaciones se calcula se realiza el ranking de candidatos posicionando primero los candidatos con mejor puntuación en el modelo del lenguaje y menor distancia de editado al OOV.\\

Se ha definido además un umbral mínimo para realizar el ranking, los candidatos que no lo cumplan con los marcadores son eliminados. Al finalizar este proceso para cada OOV se selecciona el mejor candidato del ranking, y se anota como Variation, y si no tuviera candidatos, debido al umbral, se anotan como NoEs.

\subsection{Postproceso}\label{sec:postproceso}
Esta última fase consiste en poner mayúsculas en las palabras que fueran necesarias, así como signos de exclamación, interrogación y puntuación.

\section{Implementación}\label{sec:implementacion}
En esta sección se pretende explicar la implementación software que se ha realizado de la solución. Mencionando los lenguajes y recursos externos utilizados, y explicando el funcionamiento de la aplicación web desarrollada. También se puede encontrar toda la documentación generada del proyecto al final de este documento (\hyperref[sec:tweetsccorejavadoc]{sección 9}, \hyperref[sec:tweetscwebjavadoc]{sección 10}, \hyperref[sec:tweetscexecutablejavadoc]{sección 11}).

\subsection{Diseño de arquitectura}\label{sec:disenodearquitectura}
La implementación se ha realizado en tres módulos o componentes. Por una parte tenemos la biblioteca con la funcionalidad necesaria para corregir textos de Twitter, acceder a su API y evaluar los resultados sobre un corpus de tweets; después un modulo que implementa aplicación web y por último otro que ofrece funcionalidad para utilizar la biblioteca desde línea de comandos.\\

El lenguaje principal utilizado en todo el proyecto ha sido Java, con la excepción de Python para los script de evaluación y normalización de archivos de datos, y he hecho uso de Google Cloud Engine \cite{googlecloudengine} para que la aplicación web esté disponible para cualquier usuario \cite{tweetscweb:spellchecker}.\\

Nuestro sistema software se ha intentado desarrollar de forma que sea un sistema de procesamiento dinámico pudiendo añadir y quitar funcionalidad de manera sencilla, cualquier algoritmo o método implementado funciona a partir de una clase superior para que se puedan añadir nuevos métodos.\\

El módulo principal del sistema es TweetSCCore que implementa la biblioteca para corregir tweets, acceder a la API de Twitter y evaluar resultados mediante el script ofrecido por Tweet-Norm 2013 \cite{alegria:2013}. Los módulos que funcionan a partir de TweetSCCore son: TweetSCExecutable que implementa la funcionalidad necesaria para ejectar nuestro sistema a partir de línea de comandos, y TweetSCWeb donde se implementa la aplicacion web.\\

Se han implementado dos métodos de corrección o normalización de tweets a los que he llamado DictionaryMethod, método preliminar que hace uso de diccionarios y la regla de Bayes, y TweetSCMethod que es nuestro método final con las fases que han sido explicadas.

Los módulos que forman el sistema son los siguientes:
\begin{itemize}
	\item Tokenizer: Recibe el texto sin normalizar y lo tokeniza devolviendo una lista de tokens (\hyperref[sec:tokenizacion]{sección 3.1}).
	\item ApplyRules: Aplica las reglas de preprocesado a partir del archivo de reglas, recibe una lista de tokens y devuelve dos listas (OOV detectados con las reglas y tokens que no son OOV) (\hyperref[sec:reglasdepreprocesado]{sección 3.2})
	\item OOVDetector: Detecta, a partir de una lista de tokens de entrada, palabras OOV utilizando los diccionarios de palabras en español, palabras en inglés y entidades. Además de detectar palabras OOV, las que son detectadas se anotan como se especifica en la tarea Tweet Norm 2013 \cite{alegria:2013}. Las palabras OOV que son anotadas como correctas o que no son en español pasan a la lista definitiva de OOV, el resto pasan al siguiente módulo (\hyperref[sec:detecciondeoov]{sección 3.3}).
	\item Annotator: Forma parte del módulo OOVDetector y se utiliza para anotar las palabras OOV según la tarea Tweet Norm 2013 \cite{alegria:2013}.
	\item CandidatesGenerator: Genera los candidatos para cada palabra OOV según los métodos que se definan: LevenshteinFST, Metaphone, L(\_L)+ o fastText (\hyperref[sec:generaciondecandidatosoov]{sección 3.4}).
	\item CandidatesRanking: Aplica un ranking a los candidatos de cada OOV para seleccionar el de mayor puntuación (\hyperref[sec:rankingdecandidatos]{sección 3.5}).
	\item PostProceso: Último módulo en aplicarse del sistema, al texto final normalizado se le aplican reglas de postproceso para añadir signos de puntuación y mayúsculas (\hyperref[sec:postproceso]{sección 3.6}).
\end{itemize}
Todos estos módulos pertenecen al paquete tweetsccore \footnote{\url{https://github.com/jmorenov/TweetSC/tree/master/code/tweetsccore}}.

\begin{figure}[h]
 \includegraphics[width=1.2\textwidth]{recursos/DiagramaDelSistema.png}
\caption{Diagrama de módulos del sistema}
\label{fig:diagramadelsistema}
\end{figure}

\newpage
\subsection{Instrucciones para usar el software}\label{sec:instruccionesparausarelsoftware}
Para utilizar el software desarrollado primero es necesario tener instalado Git y Java 1.8. Después de bajar el código fuente: 
\begin{verbatim}
git clone https://github.com/jmorenov/TweetSC
\end{verbatim}
Posteriormente se compila el código: 
\begin{verbatim}
cd TweetSC/code/
chmod +x build_all.sh
./build_all.sh
\end{verbatim}
Para ejecutarlo desde línea de comandos:
\begin{verbatim}
java -jar tweetscexecutable-all-v0.5.0-alpha.jar -text Texto de prueba
\end{verbatim}
La ejecución de la evaluación sobre el corpus de Tweet-Norm 2013 \cite{alegria:2013}
\begin{verbatim}
java -jar tweetscexecutable-all-v0.5.0-alpha.jar \
    -workingDirectory evaluation \
    -annotatedFile tweet-norm-dev500_annotated.txt \
    -tweetsFile tweet-norm-dev500.txt \
    -resultFile results-test-dev500.txt \
    -method TweetSCMethod
\end{verbatim}
La ejecución de la aplicación web:
\begin{verbatim}
cd tweetscweb
./gradlew run
\end{verbatim}

\subsection{Aplicación web}\label{sec:aplicacionweb}
En esta sección se explicará el funcionamiento de la aplicación web desarrollada y se mostrarán capturas de pantalla de la misma en funcionamiento.\\

La aplicación web se encuentra en nuestro paquete TweetSCWeb y hace uso del framework Spring Boot. Para el backend se utiliza Java y para el frontend Javascript (Jquery). Se ha intentado realizar un diseño sencillo y fluido usando la biblioteca Bootstrap.\\

\begin{figure}[h]
El funcionamiento de la aplicación web es muy sencillo, tiene una sección principal desde la que se accede a las demás secciones mediante una barra de navegación.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebInicio.png}
\caption{Inicio de la aplicación web}
\label{fig:webinicio}
\end{center}
\end{figure}

\begin{figure}[h]
En la sección siguiente se puede ver un corrector de texto simple.
\begin{center}
\includegraphics[width=0.9\textwidth]{recursos/WebUseIt.png}
\caption{Sección para utilizar el corrector}
\label{fig:webuseit}
\end{center}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_Corrected.png}
\caption{Ejemplo de texto corregido}
\label{fig:webuseitcorrected}
\end{figure}

\begin{figure}[h]
Si accedemos al corrector de uso avanzado podemos buscar tweets mediante texto, usuarios o id del tweet.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse.png}
\caption{Corrector de tweets uso avanzado}
\label{fig:webuseitadvanceduse}
\end{center}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_Loading.png}
\caption{Ejemplo de búsqueda de tweets}
\label{fig:webuseitadvanceduseloading}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_SearchResult.png}
\caption{Ejemplo de tweets encontrados}
\label{fig:webuseitadvancedusesearchresult}
\end{figure}

\begin{figure}[h]
Los tweets encontrados se pueden seleccionar para corregirlos.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_TweetsSelected.png}
\caption{Ejemplo de selección de tweets para corregir}
\label{fig:webuseitadvancedusetweetselected}
\end{center}
\end{figure}

\begin{figure}[h]
Cuando los tweets son corregidos se marcan y se muestran ambas versiones, normalizada y la inicial.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebUseIt_AdvancedUse_TweetCorrected.png}
\caption{Ejemplo de tweets corregidos}
\label{fig:webuseitadvancedusetweetcorrected}
\end{center}
\end{figure}

\begin{figure}[h]
La siguiente sección es la de características del sistema.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebFeatures1}
\caption{Sección de características de la aplicación web}
\label{fig:webfeatures1}
\end{center}
\end{figure}

\begin{figure}[h]
\centering
 \includegraphics[width=0.9\textwidth]{recursos/WebFeatures2}
\caption{Sección de características de la aplicación web}
\label{fig:webfeatures2}
\end{figure}

\begin{figure}[h]
Además se ha añadido una sección para colaborar mediante GitHub en el desarrollo.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebColaborate}
\caption{Sección de colaboración}
\label{fig:webcolaborate}
\end{center}
\end{figure}

\begin{figure}[h]
Por último la sección de contacto.
\begin{center}
 \includegraphics[width=0.9\textwidth]{recursos/WebContact}
\caption{Sección de contacto}
\label{fig:webcontact}
\end{center}
\end{figure}

\section{Recursos utilizados}\label{sec:recursosutilizados}
Los recursos utilizados por nuestro sistema son variados, desde diccionarios hasta bibliotecas para el desarrollo. Empezando desde el paquete TweetSCCore, se ha utilizado la biblioteca StanfordNLP \cite{stanfordnlp} para la tokenización, además está disponible en el paquete la biblioteca Freeling \cite{freeling}. El acceso a la API de Twitter se realiza mediante la bibliteca para Java Twitter4j. La detección de OOV utiliza tres diccionarios, el diccionario de español proporcionado por la herramienta Aspell, el diccionario de entidades JRC y un diccionario de inglés \cite{englishdictionary}. Los métodos de la generación de candidatos utilizan los recursos: Algoritmo del metáfono \cite{mosquera:2011}, fastText \cite{facebook:fasttext} y la biblioteca liblevenshtein \cite{liblevenshtein} para el FST. El ranking de candidatos utiliza la biblioteca \cite{opennlp} para el modelo del lenguaje N-Grama y la distancia Damerau-Levenshtein ofrecida por String.Util de java.\\

Además el corpus de tweets para la evaluación es el que recopiló Tweet-Norm 2013 \cite{alegria:2013} y su script en Python para los resultados.\\

En la versión final se han utilizado cuatro diccionarios:
\begin{itemize}
	\item Diccionario modificado y basado en GNU Aspell \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/aspellNormalized.dict}}.
	\item Diccionario fonético de las palabras del diccionario anterior \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/aspellNormalizedPhonetic.dict}}.
	\item Diccionario de palabras en inglés \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/english.txt}}.
	\item Diccionario de entidades modificado y basado en JRC Entities \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/jrc_entities.txt}}.
\end{itemize}

También se ha utilizado un archivo de reglas de preproceso creado especificamente para este sistema \footnote{\url{https://github.com/jmorenov/TweetSC/blob/master/code/tweetsccore/src/main/resources/preprocess/rules.txt}}.\\

El paquete TweetSCWeb utiliza el framework Spring Boot para la aplicación web junto con el sdk de Google Cloud para ofrecer la aplicación en la nube.

\section{Evaluación}\label{sec:evaluacion}
Esta sección describe la evaluación que se ha hecho de la solución propuesta. Primero se define la metodología utilizada para evaluar la solución, en segundo lugar explicamos el corpus utilizado como datos de entrada, posteriormente el gold standard actual y por último los experimentos que he realizada con sus resultados.

\subsection{Metodología}\label{sec:metodologia}
La metodología que he seguido ha sido la misma que en la tarea compartida Tweet-Norm 2013 \cite{tweetnorm}. Ellos utilizan como medida de evaluación la corrección de errores, sólo tiene en cuenta si la forma propuesta es correcta en base a los criterios: \textbf{correcta} si la forma original era correcta y no se ha realizado ninguna normalización o si la forma original era incorrecta y el candidato seleccionado es el correcto; \textbf{errónea} en cualquier otro caso. La evaluación final es el número de decisiones realizadas correctamente sobre el total de palabras OOV.

\subsection{Corpus}\label{sec:corpus}
El corpus utilizado es el mismo que en la tarea compartida Tweet-Norm 2013 \cite{tweetnorm}, en donde utilizan dos subconjuntos uno de desarrollo con 500 tweets y otro de evaluación con 600 tweets.

\subsubsection{Gold Standard}\label{sec:goldstandard}
Nuestro gold standard ha sido el sistema propuesto RAE \cite{porta:2013} en Tweet-Norm 2013 \cite{tweetnorm} donde consiguieron un resultado de 0.781 de precisión. Su sistema se basa en trasductores de estados finitos con pesos. 

\subsection{Experimentos}\label{sec:experimentos}
\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\textbf{Método} & \textbf{N} & \textbf{Positivos} & \textbf{Negativos} & \textbf{Errores} & \textbf{\textit{Accurancy (\%)}} & \textbf{Tiempo(s)} \\ 
\hline 
DictionaryM & 10 & 6 & 10 & 4 & 31.578 & 1.889 \\ 
\hline 
DictionaryM & 100 & 18 & 82 & 39 & 15.652 & 7.705 \\ 
\hline 
DictionaryM & 500 & 131 & 405 & 162 & 21.510 & 18.926 \\ 
\hline 
TweetSCM & 10 & 11 & 5 & 4 & 57.894 & 4.502 \\ 
\hline 
TweetSCM & 100 & 23 & 77 & 39 & 20.0 & 45.118 \\ 
\hline 
TweetSCM & 500 & 122 & 423 & 160 & 20.032 & 227.438 \\ 
\hline 
Sistema RAE & - & - & - & - & 78.1 & - \\ 
\hline 
\end{tabular}
\caption{Resultados de ejecución de mi sistema (DictionaryMethod y TweetSCMethod) y comparación con resultados del sistema RAE \cite{porta:2013} que obtuvo los mejores resultado en la tarea Tweet Norm 2013\cite{alegria:2013}.}
\label{table:results}
\end{table}

Cómo se puede observar en la tabla \ref{table:results} se muestran los resultados de mis dos métodos desarrollados, siendo DictionaryMethod la primera versión de mi sistema y consiste en un método básico de corrección con diccionarios, explicado en la \hyperref[sec:solucionpropuesta]{sección 3}. TweetSCMethod es la versión final de mi solución.\\

La ejecución se realiza sobre tres conjuntos de tweets: 10, 100 y 500. Los resultados que se ven en la tabla son: valores positivos, negativos, errores, \textit{accurancy} y tiempo de ejecución.

\section{Conclusiones y líneas futuras}\label{sec:conclusiones}
Este Trabajo de Fin de Máster ha diseñado e implementado un corrector de textos en español en Twitter. El trabajo ha tenido una componente de investigación, y en algunos aspectos se ha ido más allá del estado arte, implementando las técnicas: Word2Vec (específicamente fastText), una FST con la distancia Levenshtein, el algoritmo del metáfono para realizar similaridad fonética entre palabras y un modelo del lenguaje N-grama. El sistema se ha implementado, es plenamente funcional y su código está accesible en \url{https://github.com/jmorenov/TweetSC}. Además, es posible verlo en acción en la web \url{http://jmorenov.github.io/TweetSC/}.\\

Se puede concluir que nuestro objetivo era construir un corrector de texto para Twitter (\hyperref[sec:objetivos]{sección 1.2}), juntos con los subobjetivos, y se ha conseguido cómo se ha demostrado en las secciones \hyperref[sec:solucionpropuesta]{sección 3} y \hyperref[sec:implementacion]{sección 4}.\\

Los resultados en los experimentos han sido de menor calidad que el sistema RAE con el que se han comparado \cite{porta:2013}, un x\% inferiores en \textit{accurancy} con el conjunto de 500 tweets. Tras observar los errores detalladamente en los resultados he detectado que la mayoría de ellos proviene en la detección de OOV (sobretodo en palabras de habla inglesa o anglicismos) y en el ranking (la función de distancia y el modelo del lenguaje no tiene un funcionamiento óptimo).\\

Este sistema tiene proyección para mejorar estos resultados debido a que se ha diseñado de forma que sea posible añadir nuevos métodos tanto para generar candidatos cómo para realizar el ranking de los mismos o detectar palabras OOV. La primera mejora a realizar sería sobre el modelo del lenguaje N-grama para conseguir un buen funcionamiento del ranking.\\

Las líneas futuras son muy amplias ya que estamos en un tema bastante reciente, sobre todo en español cómo se puede ver en el \hyperref[sec:estadodelarte]{sección 2}, y los resultados se pueden mejorar de bastantes formas. Centrándonos en nuestro sistema una mejora futura sería el añadir contexto a los tweets a partir de sus \textit{hashtags}, usuarios, imágenes o emoticonos; de forma que se pudiera reducir el conjunto de candidatos o añadir nuevos a partir de estos datos. También se podría realizar un análisis de sentimientos sobre el tweet después de normalizar por si se pudiera mejorar la corrección de algún OOV.

\section{TweetSCCore Documentación del código (Java Documentation)}\label{sec:tweetsccorejavadoc}
\input{tweetsccore_javadoc}

\section{TweetSCWeb Documentación del código (Java Documentation)}\label{sec:tweetscwebjavadoc}
\input{tweetscweb_javadoc}

\section{TweetSCExecutable Documentación del código (Java Documentation)}\label{sec:tweetscexecutablejavadoc}
\input{tweetscexecutable_javadoc}

\section{ANEXOS}

\subsection{Glosario de términos}\label{sec:glosariodeterminos}
\begin{itemize}
	\item \textbf{Modelo (estadístico) del lenguaje:} Un modelo estadístico del lenguaje es una distribución de probabilidad sobre secuencias de palabras. Un tipo de modelo del lenguaje es el unigrama, también se suele llamar modelo de bolsa de palabras. La dispersidad en los datos es un problema al construir modelos del lenguaje. La secuencia de palabras más probable puede no aparecer en los datos de entrenamiento. Una solución es realizar la suposición de que la probabilidad de una palabra sólo depende de las n palabras previas. Esto es conocido como el modelo n-grama, unigrama cuando n=1. Los modelos del lenguaje neuronales o modelos del lenguaje continuos: modelo del lenguaje Skip-gram, base de word2vec.
	\item \textbf{Modelo del lenguaje N-grama:} Un modelo de n-grama es un tipo de modelo probabilístico que permite hacer predicción estadística del próximo elemento de cierta secuencia de elementos sucedida hasta el momento. Un modelo de n-grama puede ser definido por una cadena de Márkov de orden n-1. Predice $x_i$ basándose en los n elementos anteriores. 
	\item \textbf{Cadena de Márkov:} En la teoría de la probabilidad, se conoce como cadena de Márkov o modelo de Márkov a un tipo especial de proceso estocástico discreto en el que la probabilidad de que ocurra un evento depende solamente del evento inmediatamente anterior. En matemáticas se define como un proceso estocástico discreto que cumple con la propiedad de Márkov, es decir, si se conoce la historia del sistema hasta su instante actual, su estado presente resume toda la información relevante para describir en probabilidad su futuro.
	\item \textbf{Proceso de Márkov:} Fenómeno aleatorio dependiente del tiempo para el cual se cumple la propiedad de Márkov. Frecuentemente el término cadena de Márkov se usa para dar a entender que un proceso de Márkov tiene un espacio de estados discreto (infinito o numerable).
	\item \textbf{Modelo oculto de Márkov:} Un modelo oculto de Márkov (Hidden Markov Model, HMM) es un modelo estadístico en el que se asume que el sistema a modelar es un proceso de Márkov de parámetros desconocidos. El objetivo es determinar los parámetros desconocidos (u ocultos) de dicha cadena a partir de los parámetros observables. Un HMM se puede considerar como la red bayesiana más simple. 
	\item \textbf{Etiquetado gramatical:} El etiquetado gramatical (part-of-speech tagging, POS tagging o POST) se considera el proceso de asignar a cada palabra de un texto su categoría gramatical. Las soluciones se pueden dividir en dos grandes grupos: aproximaciones lingüísticas basadas en un conjunto de reglas establecidas manualmente por expertos aprendidas de forma (semi)automática, y las aproximaciones de aprendizaje automático que usan textos, generalmente anotados, para establecer los modelos. Además se pueden encontrar aproximaciones híbridas que combinan ciertos aspectos de las anteriores.

\end{itemize}

%	REFERENCIAS
\bibliographystyle{plain}
\bibliography{References}

\end{document}