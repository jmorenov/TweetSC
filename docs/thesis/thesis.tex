\documentclass[spanish,12pt, a4paper,twoside]{paper}

\let\oldsection\section
\def\section{\cleardoublepage\oldsection}

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\usepackage[textwidth=15cm, textheight=22.5cm, top=3.5cm, bottom=3.5cm,left= 4cm,right=2cm]{geometry}


\usepackage[spanish, activeacute]{babel}
%\usepackage[applemac]{inputenc} 
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{changepage}
\usepackage{subcaption}

\usepackage{url}
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{algorithm}
\usepackage{multirow}
\begin{document}
%\maketitle
%\thispagestyle{empty}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%	HEADING SECTIONS
\includegraphics[width=2.25cm]{recursos/logoFi.png}
  \hspace{8cm}
\includegraphics[width=2cm]{recursos/logoupm.png}
\\[1cm]

\textsc{\Large Escuela Técnica Superior de Ingenieros Informáticos}\\[0.5cm]
\textsc{\large Universidad Polítecnica de Madrid}
\\[3cm]


%	TITLE SECTION
 \HRule \\[0.4cm]
{ \huge \bfseries TweetSC: Corrector de texto para twitter}\\[0.4cm] % Title of your document
\HRule \\[2.5cm]

\textsc{\LARGE Trabajo Fin de Máster}\\[0.5cm] 
\textsc{\Large Máster Universitario en Inteligencia Artificial }\\[2.5cm]

 %	AUTHOR SECTION
\begin{flushright}
\large
AUTOR: Javier Moreno Vega\\
TUTOR/ES: Óscar Corcho García y \linebreak
                    Víctor Rodríguez Doncel
\end{flushright}

%\vspace{0.5cm}

%	URL SECTION
{{\url{https://jmorenov.github.io/TweetSC/}}}\\[1cm]

%\vspace{0.5cm}

%	DATE SECTION
{ {\today}}\\[1cm]

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\afterpage{\blankpage}
\pagenumbering{roman}

%	RESUMEN
\section*{RESUMEN}
Extensión máxima de una página


%	SUMMARY
\section*{SUMMARY}
Extensión máxima de una página


%	ÍNDICE
\tableofcontents % indice de contenidos



%	INDICE DE FIGURAS Y TABLAS
\listoffigures
\listoftables



%	CAPÍTULOS DEL TRABAJO FIN DE MÁSTER
\newpage
\pagenumbering{arabic} 

\section{Introducción}\label{sec:introduccion}
\subsection{Motivación}\label{sec:motivacion}
Los nuevos sistemas de comunicación como la mensajería instantánea, chats, redes sociales han generado un uso diferente de los idiomas en estos ámbitos, llamado lenguaje tipo chat \cite{forsyth:2007}. Una de estas redes sociales y en la que este trabajo va a centrarse es Twitter. En esta red social predomina el Uso de emoticonos, repetición de vocales o eliminación de las mismas, uso abusivo de mayúsculas o asusencia, siglas de expresiones populares; lo que dificulta el análisis de los textos. Las ventajas que ofrece esta red social para investigar sobre ella son la cantidad de datos en tiempo real y su fácil acceso.\\\\Uno de los principales problemas a la hora de analizar textos procedentes de las redes sociales son los errores gramaticales que suelen contener, así como la presencia de elementos propios de este tipo de foros que requieren de un procesamiento especial (i.e. hashtags, formas de mencionar a otros usuarios o emoticonos y expresiones habituales en las redes). Además, la limitación en el número de caracteres existente en Twitter la convierte en un caso singular dentro de las redes sociales, ya que los usuarios tienden a adaptar su forma de escribir a dicha limitación, omitiendo palabras y creando abreviaturas que dificultan el uso de herramientas genéricas de procesamiento del lenguaje, especialmente a la hora de realizar tareas como el Análisis de Sentimientos.\\\\Los usuarios en twitter tienden a cometer errores tipográficos, abreviaciones, sustituciones fonéticas y estructuras no gramaticales en los mensajes cortos de texto, causando problemas en las herramientas de análisis. Esto es lo que se consideran palabras mal formadas y la detección de las palabras mal formadas es difícil debido al contexto ruidoso. El objetivo es normalizar estas palabra mal formadas.\\A parte de un uso puramente de investigación, este tipo de trabajo también es beneficioso para un estudio de marcas o personas y sobre lo que las persones opinan sobre ello en las redes social, ya que sin el proceso de normalización y análisis de sentimientos estariamos ante millones de datos que costarían mucho trabajo analizar de una forma automática.

\subsection{Objetivos}\label{sec:objetivos}
El objetivo principal de este trabajo es la creación de un corrector que "normalice" tweets en español.\\Para cumplir con este objetivo principal se ha dividido en los siguientes subobjetivos.
\begin{itemize}
\item Corregir tweets (palabras y gramaticalmente)
\item Procesar emoticonos deduciendo su significado en el contexto de análisis de sentimientos
\item Expandir hashtags
\item Procesar las conversaciones de los tweets, así como las URLs o imágenes para añadir contexto
\end{itemize}
Estos subojetivos se cumplirán con su implementación en un módulo software que además estará disponible en una aplicación web \cite{tweetscweb}.

\subsection{Resumen del documento}\label{sec:resumen}
Esta memoria explica todo el trabajo desarrollado entrando en detalle en el estado del arte y el  módulo software desarrollado.\\\\
Primero se expone el estado del arte desarrollado sobre el tema de la corrección de textos, específicamente en twitter y en español.\\\\
En segundo lugar se presenta el análisis realizado, atendiendo a: metodologías de desarrollo utilizadas, análisis de requisitos, solución propuesta.\\\\
Posteriormente se plantea la implementación que se ha seguido entrando en detalle en cómo usar el software y en detalles técnicos.\\\\
Por último se muestran unas conclusiones, incluyendo un resumen del trabajo desarrollado y los objetivos conseguidos.
\section{Estado del arte}\label{sec:estadodelarte}
\subsection{Introducción}\label{sec:introduccion}
En la actualidad, la normalización lingüística de tweets \cite{baldwin:2011} supone un campo de gran interés y en donde la mayoría de trabajos se han realizado sobre textos en inglés y pocos en español. Además no hay ningún trabajo en donde se incluya, dentro de la normalización de tuits, el estudio de los hashtags o etiquetas y los emoticonos, y su contexto. 
Una introducción al tema de normalización de tuits es el artículo \cite{eisenstein:2013}, donde se revisa el estado del arte en NLP sobre variantes SMS y tweets, y cómo la comunidad científica ha respondido por dos caminos: normalización y adaptación de herramientas.
\subsection{Normalización}\label{sec:normalizacion}
El modelo de canal ruidoso \cite{shannon:1948} ha sido tradicionalmente la primera aproximación a la normalización de textos. Supone que el texto mal formado es T y su forma normalizada es S, por lo que hay que encontrar: arg max $P(S|T)$, calculando arg max $P(T|S) P(S)$, P(S) es el modelo del lenguaje y $P(T|S)$ es el modelo de error. \cite{brillmoore:2000} caracterizan el modelo de error calculando el producto de operaciones de probabilidad en partes de cadenas de caracteres. \cite{toutanovamoore:2002} mejoraron el modelo incorporando información de la pronunciación. \cite{choudhury:2007} modela el proceso de generación de texto a nivel de palabra para mensajes SMS considerando las abreviaturas grafémicas/fonéticas y los errores tipográficos involuntarios como transiciones de estado ocultas del modelo de Markov (HMM) y emisiones, respectivamente. \cite{cookstevenson:2009} expandieron el modelo de error introduciendo inferencias de diferentes procesos de formación erróneos, de acuerdo con la distribución de errores muestreada.\\\\Mientras el modelo de canal ruidoso es apropiado para normalización de textos, es difícil aproximar la normalización con exactitud, además estos métodos ignoran el contexto alrededor del OOV, el cual ayuda a resolver ambigüedades. La traducción automática estadística (SMT) se ha propuesto como un medio de normalización de texto sensible al contexto, al tratar el texto mal formado como el idioma de origen, y la forma estándar como el idioma de destino. Por ejemplo \cite{aw:2006}. Normalización de textos como un problema de reconocimiento de voz \cite{kobus:2008}. \cite{beaufort:2002} métodos de estado finitos combinando las ventajas de SMS y el modelo de canal ruidoso. \cite{kaufmannkalita:2010} usan un enfoque de traducción automática con un preprocesador para la normalización sintáctica (en lugar de léxica).\\\\El problema de estos trabajos anteriores es que requieren datos de entrenamiento anotados a gran escala, lo que limita su adaptabilidad a nuevos dominios o idiomas, mientras que los trabajos \cite{whitelaw:2009} y \cite{baldwin:2011}, no. Estos trabajos son una buena referencia en el campo de la normalización de tuits en inglés de forma no supervisada. En donde para detectar palabras fuera de diccionario (OOV) utilizan GNU aspell, y los usuarios (@usuario), los hashtags y las URLs son excluidas de la normalización. La normalización tiene relación con los correctores de texto \cite{peterson:1980} pero difiere en que las palabras mal formadas en los mensajes de texto suelen ser intencionadas, para ahorrar caracteres, como identidad social, o debido a la convención en este subgénero de texto. La detección de las palabras mal formadas es difícil debido al contexto ruidoso. El objetivo es normalizar estas palabra mal formadas, además muchas palabras mal formadas son ambiguas y requieren el contexto para poder normalizarlas.

\subsection{Adaptación de herramientas}\label{sec:adaptaciondeherramientas}
En vez de adaptar el texto a herramientas de análisis otro de los caminos a seguir es adaptar las herramientas de análisis al texto. Destacan los trabajos de reconocimiento de voz \cite{gimpel:2011} \cite{owoputi:2013}, reconocimiento de entidades \cite{finin:2010} \cite{ritter:2011} \cite{liu:2011}, análisis gramatical \cite{foster:2011}, modelización de diálogos \cite{ritter:2010} y resumen automático de textos \cite{sharifi:2010}.\\\\

El reconocimiento de entidades nombradas (NER) es una tarea de extracción de información que busca localizar y clasificar en categorías predefinidas, como personas, organizaciones, lugares, expresiones de tiempo y cantidades, entidades encontradas en un texto. Las soluciones propuestas para NER suelen recaer en tres categorías: Basado en reglas \cite{krupkahausman:1998}, Basada en aprendizaje automático \cite{finkelmanning:2009} \cite{singh:2010} y Métodos híbridos \cite{jansche:2002}. Con la disponibilidad de datos anotados, Enron \cite{minkov:2005} y CoNLL03 \cite{tjong:2003} se han convertidos en los nuevos métodos dominantes. El estudio actual NER se centra principalmente en textos formales, de hecho, el estado del arte actual (CoNLL03) tiene un éxito del 90.8\% en textos formales y 45.8\% en tweets. En el contexto de los textos en Tweets, existe una dificultad en el reconocimiento de entidades nombradas debido a la falta de información y datos de entrenamiento.\\\\El trabajo en el contexto de los textos de Twitter se puede dividir en tres categorías: NER en tweets, NER en no tweets y aprendizaje semi-supervisado para NER. El trabajo principal de NER sobre tweets es \cite{finin:2010}, en donde se anotan los tweets y se entrena el modelo con CRF. En cuanto a los trabajos de NER sobre no tweets: \cite{krupkahausman:1998} utilizan reglas manuales para extraer entidades de tipos predefinidos, \cite{zhousu:2002} utilizan HMM (Hidden Markov Model) mientras que \cite{finkel:2005} usa CRF. En la tercera categoría, aprendizaje semi-supervisado para NER, se encuentran los trabajos de \cite{jiangzhai:2007} que utiliza un algoritmo de bootstrapping balanceado, \cite{wu:2009} también utiliza un algoritmo de bootstrapping, \cite{miller:2004} clusters de palabras, \cite{brown:1992} aprende desde texto sin etiquetar y \cite{guo:2009} introduce Latent Semantic Association (LSA) para NER. El trabajo más importante y actual de NER para tweets es \cite{liu:2011} donde replantea el tema de reconocimiento de entidades nombradas en corpus de tuits. Combina un clasificador KNN con CRF (Conditional Random Fields).

\subsection{Normalización en espa'nol}\label{sec:normalzacionenespanol}
Una introducción a la normalización de tuits en español es \cite{alegria:2013}\cite{alegria:2015}. Este trabajo propuso en 2013 una tarea o competición en la que los participantes proponían soluciones de normalización de tweets. Los organizadores de la competición ofrecían dos datasets de tweets ya notados uno de desarrollo y otro para test, junto con un tercero que no era público y que era usado para la última evaluación.\\\\

Las soluciones ofreciddas por los participantes se pueden dividir en dos categorías, los que utilizan generación de candidatos junto un modelo del lenguaje, y los que utilizan transductores o FSTs (Finite State Transducers). El participante que mejor accurancy consiguió, Sistema RAE \cite{porta:2013} con un 0.781, optó por la segunda categoría e implementó un siststem basado en FSTs para la tarea de normalización léxica de mensajes de Twitter en Español. El sistema desarrollado consiste en transductores que se aplican a tokens OOV. Los transductores implementan modelos de variación lingüística que generan conjuntos de candidatos acordes a un léxico. Un modelo estadístico del lenguaje se usa para obtener la secuencia de palabras más probable. El sistema tiene tres componentes principales que se aplican secuencialmente. Un analizador que ejecuta tokenización y análisis léxico sobre palabras en forma estándar y otras expresiones (números, fechas, …). Un componente que genera palabras candidatas para los tokens OOV. Un modelo estadístico del lenguaje para obtener la mejor secuencia de palabras. Y finalmente un truecaser para capitalizar correctamente las palabras asignadas a los tokens OOV. El conjunto de confusión de un token OOV se genera aplicando el algoritmo de camino mínimo a la expresión: $W \circ E \circ L$. Donde W es el automata que representa el token OOV, E es un transductor de editado que genera todas las posibles variaciones de un token, y L es un conjunto de palabras objetivo. Dentro de esta categoría se encuentran los trabajos de la tarea: \cite{ageno:2013} en donde usan una batería de módulos para generar diferentes propuestas de corrección para cada palabra desconocida. La corrección definitiva se elige por votación ponderada según la precisión de cada módulo, \cite{alegria:2013} que además utiliza un modelo para el reconocimiento de voz para la generación de candidatos y \cite{huldenfrancom:2013} presentan dos estrategias basadas en FSTs una con reglas diseñadas manualmente y la otra automática.\\\\

Entre los participantes que optaron por la primera categoría destaca \cite{ruizcuadros:2013}\cite{vicomtech} que usa reglas de preproceso, un modelo de distancias de edición adecuado al dominio y modelos de lengua para seleccionar candidatos de corrección según el contexto. Su arquitectura está formada por: preproceso basado en expresiones regulares y listas customizadas, generación de candidatos mediante una técnica de mínima de distancia de editado, ranking de candidatos mediante una combinación con pesos de la puntuación del modelo del lenguaje y la distancia de editado y la puntuación del modelo de lenguaje es n-grama utilizando la distancia Levenshtein. El sistema obtuvo resultados superiores a la media en la tarea. Una mejora a este trabajo por los mismos autores es \cite{ruizcuadros:2014} en donde utilizan un sistema basado en reglas para seleccionar los candidatos. Otros trabajos en esta categoría son: \cite{gamallo:2013} que propone un sistema basado en \cite{han:2013}, \cite{saralegi:2013} y \cite{mosqueralopezmoreda:2013} que emplea técnicas de RAH (reconocimiento del habla) mediante la herramienta TENOR \cite{mosquera:2012} junto con un modelo del lenguaje.\\\\

Fuera de estas dos categorías nos encontramos con los trabajos: \cite{montejo:2013} que utiliza conversiones basadas en reglas hasta una forma final normalizada. Después de recibir una lista con las posibles correcciones el sistema selecciona la más común acorde con una lista de palabras ordenada por frecuencia, \cite{vilares:2013} utilizan una lista de prioridad para los candidatos obtenidos y una tabla de frencuencias de palabrar para puntuarlos, \cite{han:2013} presentan una estrategia basada en búsquedas rápidas mediante una lista de frecuencias aprendida desde un corpus de tweets, \cite{munozgarcia:2013} no generan candidatos simplemente selecciona palabras OOV y las corrigen con un corrector externo y \cite{cotelo:2013} generan candidatos y seleccionan el mejor mediante una función de distancia. Una mejora a este último trabajo por parte de los autores fue \cite{cotelocruz:2015} donde añaden un modulo de puntuación para la selección de candidatos. \\\\

Otros trabajos sobre normalización en español son \cite{mosquera:2012} en donde se generan candidatos con indexación fonética y se seleccionan el candidato calculando la similaridad léxica junto con un modelo del lenguaje trigrama y \cite{oliva:2011}. Estos trabajos son principalmente sobre mensajes SMS, y no abordan la normalización de tuits en su conjunto. Dentro de la normalización en español existen otras tareas relacionadas como es la tokenización y aquí destaca el trabajo \cite{gomezhidalgo:2013} que estudia la tokenización de textos SMS.

\subsection{Word2Vec}\label{sec:word2vec}


\subsection{Análisis de sentimientos}\label{sec:analisisdesentimientos}
Un campo muy relacionado con la normalización de tuits es el análisis de sentimientos y un trabajo que realiza un estudio sobre técnicas de análisis de sentimientos de tuits en español es \cite{fernandezanta:2013}. El trabajo \cite{gamallogarcia:2013} se centra en una técnica Naive-Bayes para el análisis de sentimientos en tuits en español.

\section{Solución propuesta}\label{sec:solucionpropuesta}
La solución propuesta y a la que hemos llamado TweetSC (Tweet Spell Checker) \cite{tweetscweb} se llegó a ella a partir de varios análisis y evaluaciones que se hicieron con diversas bibliotecas y algoritmos, y todos ellos se pueden encontrar en la solución final para su uso.\\\\
En la primera versión de nuestra solución se construyó un corrector de texto sencillo basándonos en el creado por Peter Norvig \cite{peternorvig}, el cuál utiliza un diccionario para seleccionar las palabras incorrectas y las corrige mediante el teorema de Bayes usando probabilidades. Se usa la fórmula: $argmax_{c\ \in\  candidates}P(c|w)$, que mediante el teorema de Bayes es equivalente a: $argmax_{c\ \in\  candidates}P(c) P(w|c) / P(w)$, y como P(w)es igual para cada candidato c: $argmax_{c\ \in\  candidates}P(c) P(w|c)$. Esta fórmula trata de seleccionar el candidato de probabilidad máxima para cada palabra. Para calcular la probabilidad se usan dos diccionarios, uno de palabras en Español y otro de nombres propios. Se utilizó esta primera versión como punto de partida e ir creando versiones más avanzadas.\\\\
Para mejorar la versión inicial se hizo uso de la biblioteca Stanford NLP \cite{stanfordnlp}. Esta biblioteca creada por Stanford NLP Group ofrece tanto etiquetado gramatical (POS Tagging o POST) cómo deteccion de etiquetas (Named Entity Recognition o NER), y ambas funcionalidades fueron usadas para nuestra solución.\\\\
Además se ha utilizado la API ofrecida por IBM \cite{ibmwatsonnlu} para análisis de lenguaje y cómo con la biblioteca de Stanford la utilizamos tanto para POST como NER.\\\\
Tras utilizar estas dos tecnologías en la primera fase llamada preproceso, dónde se seleccionan las palabras incorrectas y se generan los candidatos, utilizaremos los Modelos de Markov (MMO) y n-gramas para seleccionar el candidato final para cada palabra incorrecta.\\\\
Cómo se utilizan varias tecnologías y algoritmos en las diferentes fases se ha utilizado un estudio y evaluación de cuál es el mejor para la solución final.\\\\
Para facilitar el uso de nuestra solución con tweets de twitter se ha definido una conexión con su API para acceder a los mismos.

\section{Implementación}\label{sec:implementacion}
En esta sección se pretende explicar toda la implementación software que se ha realizado de nuestra solución. Primero se realizará una introducción comentando lenguajes y herramientas utilizadas. Segundo unos instrucciones sobre dónde encontrar y cómo utilizar nuestro software . Y por último la documentación generada del código fuente.

\subsection{Introducción}\label{sec:introduccion}
La implementación se ha realizado en tres módulos o componentes, por una parte tenemos la biblioteca con la funcionalidad necesaria para corregir textos de twitter y acceder a su API, después un modulo que es la aplicación web y por último otro que ofrece funcionalidad para utilizar la biblioteca desde línea de comandos.\\\\
El lenguaje utilizado en todo el proyecto ha sido Java y hemos hecho uso de Google Cloud Engine \cite{googlecloudengine} para que la aplicación web esté disponible para cualquier usuario \cite{tweetscweb:spellchecker}. Además de Gradle para compilar el código.
\subsection{Cómo usarlo}\label{sec:comousarlo}
Para utilizar nuestro software primero es necesario tener instalado Git y Java 1.8. Después realizar un clonado del código fuente: 
\begin{verbatim}
git clone https://github.com/jmorenov/TweetSC
\end{verbatim}
Posteriormente se compila el código con gradle: 
\begin{verbatim}
gradlew clean build && gradlew createJar
\end{verbatim}
y para ejecutarlo:
\begin{verbatim}
java -jar org.jmorenov.tweetsc-0.3.0-alpha.jar -text "Texto de prueba"
\end{verbatim}

\subsection{Documentación del código}\label{sec:javadoc}
(Javadoc del código)

\section{Recursos utilizados}\label{sec:recursosutilizados}
Diccionario de inglés: https://github.com/dwyl/english-words

\section{Evaluación}\label{sec:evaluacion}
Esta sección describe la evaluación que se ha hecho de la solución propuesta. Primero se define la metodología utilizada para evaluar la solución, en segundo lugar explicamos el corpus utilizado como datos de entrada, posteriormente el gold standard actual y por último los experimentos que hemos realizada con sus resultados.
\subsection{Metodología}\label{sec:metodologia}
La metodología que hemos seguido ha sido la misma que en la tarea compartida Tweet-Norm 2013 \cite{tweetnorm}. Ellos utilizan como medida de evaluación la corrección de errores, sólo tiene en cuenta si la forma propuesta es correcta en base a los criterios: \textbf{correcta} si la forma original era correcta y no se ha realizado ninguna normalización o si la forma original era incorrecta y el candidato seleccionado es el correcto; \textbf{errónea} en cualquier otro caso. La evaluación final es el número de decisiones realizadas correctamente sobre el total de palabras OOV.

\subsection{Corpus}\label{sec:corpus}
El corpus utilizado es el mismo que en la tarea compartida Tweet-Norm 2013 \cite{tweetnorm}, en donde utilizan dos subconjuntos uno de desarrollo con 500 tweets y otro de evaluación con 600 tweets.

\subsubsection{Gold Standard}\label{sec:goldstandard}
Nuestro gold standard ha sido el sistema propuesto RAE \cite{porta:2013} en Tweet-Norm 2013 \cite{tweetnorm} donde consiguieron un resultado de 0.781 de precisión. Su sistema se basa en trasductores de estados finitos con pesos. 

\subsection{Experimentos}\label{sec:experimentos}
(Experimentos realizados)

\section{Conclusiones}\label{conclusiones}
El proyecto realizado está compuesto de una parte de investigación, cómo se demuestra con el estado del arte y las diferentes soluciones que se han ido realizando hasta llegar a nuestra solución final. Además de la otra parte de desarrollo e implementación de software, ofrenciéndolo para todos en código abierto y en una aplicación web \cite{tweetscweb}.\\\\
Este desarrollo software se ha dividido en tres componentes o módulos:
\begin{itemize}
	\item TweetSCCore: Núcleo del proyecto con la funcionalidad para corregir textos de twitter.
	\item TweetSCWeb: Aplicación web para corregir textos.
	\item TweetSCExecutable: Ejecutable java para corregir textos desde línea de comandos.
\end{itemize}
Se puede concluir que nuestro objetivo era construir un corrector de texto para twitter (\hyperref[sec:objetivos]{sección 1.2}) y se ha conseguido cómo se ha demostrado en las secciones \hyperref[sec:solucionpropuesta]{3}, \hyperref[sec:implementacion]{sección 4} y \hyperref[sec:implementacion]{5}.

\section{Líneas Futuras}\label{lineasfuturas}
Las líneas futuras son muy ámplias ya que estamos en un tema bastante reciente, sobre todo en español, y se los resultados se pueden mejorar de bastantes formas.

\section*{ANEXOS}

\subsection*{Glosario de términos}
\begin{itemize}
	\item \textbf{Modelo (estadístico) del lenguaje:} Un modelo estadístico del lenguaje es una distribución de probabilidad sobre secuencias de palabras. Un tipo de modelo del lenguaje es el unigrama, también se suele llamar modelo de bolsa de palabras. La dispersidad en los datos es un problema al construir modelos del lenguaje. La secuencia de palabras más probable puede no aparecer en los datos de entrenamiento. Una solución es realizar la suposición de que la probabilidad de una palabra sólo depende de las n palabras previas. Esto es conocido como el modelo n-grama, unigrama cuando n=1. Los modelos del lenguaje neuronales o modelos del lenguaje continuos: modelo del lenguaje Skip-gram, base de word2vec.
	\item \textbf{Modelo del lenguaje N-grama:} Un modelo de n-grama es un tipo de modelo probabilístico que permite hacer predicción estadística del próximo elemento de cierta secuencia de elementos sucedida hasta el momento. Un modelo de n-grama puede ser definido por una cadena de Márkov de orden n-1. Predice $x_i$ basándose en los n elementos anteriores. 
	\item \textbf{Cadena de Márkov:} En la teoría de la probabilidad, se conoce como cadena de Márkov o modelo de Márkov a un tipo especial de proceso estocástico discreto en el que la probabilidad de que ocurra un evento depende solamente del evento inmediatamente anterior. En matemáticas se define como un proceso estocástico discreto que cumple con la propiedad de Márkov, es decir, si se conoce la historia del sistema hasta su instante actual, su estado presente resume toda la información relevante para describir en probabilidad su futuro.
	\item \textbf{Proceso de Márkov:} Fenómeno aleatorio dependiente del tiempo para el cual se cumple la propiedad de Márkov. Frecuentemente el término cadena de Márkov se usa para dar a entender que un proceso de Márkov tiene un espacio de estados discreto (infinito o numerable).
	\item \textbf{Modelo oculto de Márkov:} Un modelo oculto de Márkov (Hidden Markov Model, HMM) es un modelo estadístico en el que se asume que el sistema a modelar es un proceso de Márkov de parámetros desconocidos. El objetivo es determinar los parámetros desconocidos (u ocultos) de dicha cadena a partir de los parámetros observables. Un HMM se puede considerar como la red bayesiana más simple. 
	\item \textbf{Etiquetado gramatical:} El etiquetado gramatical (part-of-speech tagging, POS tagging o POST) se considera el proceso de asignar a cada palabra de un texto su categoría gramatical. Las soluciones se pueden dividir en dos grandes grupos: aproximaciones lingüísticas basadas en un conjunto de reglas establecidas manualmente por expertos aprendidas de forma (semi)automática, y las aproximaciones de aprendizaje automático que usan textos, generalmente anotados, para establecer los modelos. Además se pueden encontrar aproximaciones híbridas que combinan ciertos aspectos de las anteriores.

\end{itemize}

%	REFERENCIAS
\bibliographystyle{plain}
\bibliography{References}

\end{document}